{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b5bf83",
   "metadata": {},
   "source": [
    "# Bag of Words Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a384b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4661ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list with the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# instantiate WordNetlemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f856bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(word):\n",
    "    \"\"\"Tag each word with its part of speech.\"\"\"\n",
    "    \n",
    "    # get the already tagged synonyms of the word \n",
    "    probable_pos = wordnet.synsets(word)\n",
    "    # instantiate Counter()\n",
    "    pos_counts = Counter()\n",
    "    \n",
    "    # count the POS of the word's synonyms\n",
    "    pos_counts[\"n\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"n\"])\n",
    "    pos_counts[\"v\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"v\"])\n",
    "    pos_counts[\"a\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"a\"])\n",
    "    pos_counts[\"r\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"r\"])\n",
    "    \n",
    "    # find the most common POS of the word's synonyms\n",
    "    most_likely_pos = pos_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return most_likely_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92fc8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    1. Strips the text off punctuation.\n",
    "    2. Lower-case letters\n",
    "    3. Tokenize letters\n",
    "    4. Lemmatize letters\n",
    "    \"\"\"\n",
    "    # strip text off punctuation and lower-case letters\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    # tokenize text\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    # lemmatize text with POS\n",
    "    normalized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6baca8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 2, 'love': 1, 'fantastic': 2, 'fly': 2, 'fish': 3, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1}\n"
     ]
    }
   ],
   "source": [
    "def text_to_bow(some_text):\n",
    "    \"\"\"\n",
    "    Applies a Bag-of-Words language model to text.\n",
    "    \"\"\"\n",
    "    bow_dictionary = {}\n",
    "    # remove punctuation, lower-case, tokenize, lemmatize\n",
    "    tokens = preprocess_text(some_text)\n",
    "    for token in tokens:\n",
    "        if token in bow_dictionary:\n",
    "          bow_dictionary[token] += 1\n",
    "        else:\n",
    "          bow_dictionary[token] = 1\n",
    "    return bow_dictionary\n",
    "\n",
    "print(text_to_bow(\"\"\"I love fantastic flying fish. These flying fish are just ok,\n",
    "            so maybe I will find another few fantastic fish...\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bc2d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fish': 3, 'i': 2, 'fantastic': 2, 'fly': 2, 'love': 1, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text = \"\"\"I love fantastic flying fish. These flying fish are just ok,\n",
    "so maybe I will find another few fantastic fish...\"\"\"\n",
    "# remove punctuation, lower-case, tokenize, lemmatize\n",
    "tokens = preprocess_text(text)\n",
    "\n",
    "print(Counter(tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9356d0",
   "metadata": {},
   "source": [
    "When a BoW dictionary is not enough we convert the text into **BoW vectors**:\n",
    "1. Create a **features dictionary** ('word': index)\n",
    "2. Create a **BoW vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "994259d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}\n"
     ]
    }
   ],
   "source": [
    "def create_features_dictionary(documents):\n",
    "    \"\"\"Create a features dictionary from documents.\"\"\"\n",
    "    \n",
    "    features_dictionary = {}\n",
    "    # join all text into one string with a space seperator\n",
    "    merged = \" \".join(documents)\n",
    "    # remove punctuation, lower-case, tokenize, lemmatize\n",
    "    tokens = preprocess_text(merged)\n",
    "    index = 0\n",
    "    for token in tokens:\n",
    "        if token not in features_dictionary:\n",
    "          features_dictionary[token] = index\n",
    "          index += 1\n",
    "    return features_dictionary, tokens\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\",\n",
    "                          \"Maybe find another five fantastic fish?\",\n",
    "                          \"Find my fish with a function please!\"]\n",
    "\n",
    "# print features dictionary\n",
    "print(create_features_dictionary(training_documents)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e3a766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def text_to_bow_vector(some_text, features_dictionary):\n",
    "    \"\"\"Convert text and features dictionary to a Bag-of-Words vector.\"\"\"\n",
    "    \n",
    "    # create a list of 0s the length of features dictionary\n",
    "    bow_vector = [0] * len(features_dictionary)\n",
    "    # remove punctuation, lower-case, tokenize, lemmatize\n",
    "    tokens = preprocess_text(some_text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        feature_index = features_dictionary[token]\n",
    "        bow_vector[feature_index] += 1\n",
    "    return bow_vector, tokens\n",
    "\n",
    "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(text_to_bow_vector(text, features_dictionary)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78e7265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\",\n",
    "                      \"Maybe find another five fantastic fish?\",\n",
    "                      \"Find my fish with a function please!\"]\n",
    "\n",
    "test_text = [\"Another five fish find another faraway fish.\"]\n",
    "\n",
    "# instantiate CountVectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "# fit CountVectorizer to the training data to generate a features dict\n",
    "bow_vectorizer.fit(training_documents)\n",
    "# transform features dict into a vector\n",
    "bow_vector = bow_vectorizer.transform(test_text)\n",
    "\n",
    "print(bow_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c760f8a9",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Term Frequecy (tf-idf)\n",
    "1. Term Frequency: how often a word appears in a document (same as BoW)\n",
    "2. Inverse Document Frequency: a measure of how often a word appears in the overall corpus. \n",
    "\n",
    "By penalizing the score of words that appear throughout a corpus, tf-idf can give better insight into how important a word is to a particular document of a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81b15bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_1 = '''\n",
    "Success is counted sweetest\n",
    "By those who ne'er succeed.\n",
    "To comprehend a nectar\n",
    "Requires sorest need.\n",
    "\n",
    "Not one of all the purple host\n",
    "Who took the flag to-day\n",
    "Can tell the definition,\n",
    "So clear, of victory,\n",
    "\n",
    "As he, defeated, dying,\n",
    "On whose forbidden ear\n",
    "The distant strains of triumph\n",
    "Break, agonized and clear!'''\n",
    "\n",
    "poem_2 = '''\n",
    "Wild nights! Wild nights!\n",
    "Were I with thee,\n",
    "Wild nights should be\n",
    "Our luxury!\n",
    "\n",
    "Futile the winds\n",
    "To a heart in port, —\n",
    "Done with the compass,\n",
    "Done with the chart.\n",
    "\n",
    "Rowing in Eden!\n",
    "Ah! the sea!\n",
    "Might I but moor\n",
    "To-night in thee!'''\n",
    "\n",
    "poem_3 = '''\n",
    "I'm nobody! Who are you?\n",
    "Are you nobody, too?\n",
    "Then there 's a pair of us — don't tell!\n",
    "They 'd banish us, you know.\n",
    "\n",
    "How dreary to be somebody!\n",
    "How public, like a frog\n",
    "To tell your name the livelong day\n",
    "To an admiring bog!'''\n",
    "\n",
    "poem_4 = '''\n",
    "I felt a funeral in my brain,\n",
    "   And mourners, to and fro,\n",
    "Kept treading, treading, till it seemed\n",
    "   That sense was breaking through.\n",
    "\n",
    "And when they all were seated,\n",
    "   A service like a drum\n",
    "Kept beating, beating, till I thought\n",
    "   My mind was going numb.\n",
    "\n",
    "And then I heard them lift a box,\n",
    "   And creak across my soul\n",
    "With those same boots of lead, again.\n",
    "   Then space began to toll\n",
    "\n",
    "As all the heavens were a bell,\n",
    "   And Being but an ear,\n",
    "And I and silence some strange race,\n",
    "   Wrecked, solitary, here.'''\n",
    "\n",
    "poem_5 = '''\n",
    "Hope is the thing with feathers\n",
    "That perches in the soul,\n",
    "And sings the tune without the words,\n",
    "And never stops at all,\n",
    "\n",
    "And sweetest in the gale is heard;\n",
    "And sore must be the storm\n",
    "That could abash the little bird\n",
    "That kept so many warm.\n",
    "\n",
    "I 've heard it in the chillest land,\n",
    "And on the strangest sea;\n",
    "Yet, never, in extremity,\n",
    "It asked a crumb of me.'''\n",
    "\n",
    "poem_6 = '''\n",
    "The pedigree of honey\n",
    "Does not concern the bee;\n",
    "A clover, any time, to him\n",
    "Is aristocracy.'''\n",
    "\n",
    "poems = [poem_1, poem_2, poem_3, poem_4, poem_5, poem_6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13499b",
   "metadata": {},
   "source": [
    "## Part I: Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "693a3188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Poem 1  Poem 2  Poem 3  Poem 4  Poem 5  Poem 6\n",
      "abash         0       0       0       0       1       0\n",
      "across        0       0       0       1       0       0\n",
      "admire        0       0       1       0       0       0\n",
      "again         0       0       0       1       0       0\n",
      "agonize       1       0       0       0       0       0\n",
      "...         ...     ...     ...     ...     ...     ...\n",
      "word          0       0       0       0       1       0\n",
      "wreck         0       0       0       1       0       0\n",
      "yet           0       0       0       0       1       0\n",
      "you           0       0       3       0       0       0\n",
      "your          0       0       1       0       0       0\n",
      "\n",
      "[173 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# preprocess text\n",
    "processed_poems = [preprocess_text(poem) for poem in poems]\n",
    "\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform(processed_poems)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies (term-doc matrix)\n",
    "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(df_term_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffd96f",
   "metadata": {},
   "source": [
    "## Part II: Inverse Document Frequency\n",
    "The intuition is that words that appear more frequently in the corpus give less insight into the topic or meaning of an individual document, and should thus be deprioritized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c8042c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Poem 1  Poem 2    Poem 3    Poem 4    Poem 5  Poem 6\n",
      "abash    0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "across   0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "admire   0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "again    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "agonize  2.252763     0.0  0.000000  0.000000  0.000000     0.0\n",
      "...           ...     ...       ...       ...       ...     ...\n",
      "word     0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "wreck    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "yet      0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "you      0.000000     0.0  6.758289  0.000000  0.000000     0.0\n",
      "your     0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "\n",
      "[173 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(processed_poems)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b50b8",
   "metadata": {},
   "source": [
    "## Converting BoW into tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5aed3072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Poem 1  Poem 2  Poem 3  Poem 4  Poem 5  Poem 6\n",
      "abash         0       0       0       0       1       0\n",
      "across        0       0       0       1       0       0\n",
      "admire        0       0       1       0       0       0\n",
      "again         0       0       0       1       0       0\n",
      "agonize       1       0       0       0       0       0\n",
      "...         ...     ...     ...     ...     ...     ...\n",
      "word          0       0       0       0       1       0\n",
      "wreck         0       0       0       1       0       0\n",
      "yet           0       0       0       0       1       0\n",
      "you           0       0       3       0       0       0\n",
      "your          0       0       1       0       0       0\n",
      "\n",
      "[173 rows x 6 columns]\n",
      "           Poem 1  Poem 2    Poem 3    Poem 4    Poem 5  Poem 6\n",
      "abash    0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "across   0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "admire   0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "again    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "agonize  2.252763     0.0  0.000000  0.000000  0.000000     0.0\n",
      "...           ...     ...       ...       ...       ...     ...\n",
      "word     0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "wreck    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
      "yet      0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
      "you      0.000000     0.0  6.758289  0.000000  0.000000     0.0\n",
      "your     0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
      "\n",
      "[173 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(processed_poems)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "df_bag_of_words = pd.DataFrame(bow_matrix.T.todense(), index=feature_names, columns=corpus_index)\n",
    "\n",
    "# display term-document matrix of term frequencies (bag-of-words)\n",
    "print(df_bag_of_words)\n",
    "\n",
    "# initialize and fit TfidfTransformer, transform bag-of-words matrix\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "tfidf_scores = transformer.fit_transform(bow_matrix)\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index = feature_names, columns=corpus_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
