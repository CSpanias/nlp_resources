{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee77992",
   "metadata": {},
   "source": [
    "# Retrieval-Based ChatBot\n",
    "The most popular chatbot implementation in use today!\n",
    "\n",
    "Retrieval-Based ChatBots perform **three main tasks**:\n",
    "\n",
    "1. [**Intent Classification**](#IntentClassification)  \n",
    "Classify the intent of the message from user input.\n",
    "    1. [Intent Classification with Bag-of-Words](#ICBoW) \n",
    "\n",
    "\n",
    "2. [**Entity Recognition**](#EntityRecognition)  \n",
    "Entities are often the proper nouns of a message.\n",
    "    1. [Entity Recognition with Part-of-Speech tagging](#ERPOS)\n",
    "    2. [Entity Recognition with Word Embeddings](#ERWE)\n",
    "\n",
    "\n",
    "3. [**Response Selection**](#ResponseSelection)  \n",
    "Retrieve the best-fit response from this collection\n",
    "\n",
    "\n",
    "4. [**Bringing them all together!**](#ChatBot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "754ca518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def pos_tagging(word):\n",
    "    \"\"\"Tag each word with its part of speech.\"\"\"\n",
    "    \n",
    "    # get the already tagged synonyms of the word \n",
    "    probable_pos = wordnet.synsets(word)\n",
    "    # instantiate Counter()\n",
    "    pos_counts = Counter()\n",
    "    \n",
    "    # count the POS of the word's synonyms\n",
    "    pos_counts[\"n\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"n\"])\n",
    "    pos_counts[\"v\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"v\"])\n",
    "    pos_counts[\"a\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"a\"])\n",
    "    pos_counts[\"r\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"r\"])\n",
    "    \n",
    "    # find the most common POS of the word's synonyms\n",
    "    most_likely_pos = pos_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return most_likely_pos\n",
    "\n",
    "# create a set with the english stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    1. Strips the text off punctuation.\n",
    "    2. Lower-case letters\n",
    "    3. Splits sentences into tokens\n",
    "    4. Removes stopwords\n",
    "    4. Lemmatize tokens\n",
    "    \"\"\"\n",
    "    # strip text off punctuation and lower-case letters\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    # tokenize text\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    # remove stopwords\n",
    "    no_stop = [token for token in tokenized if token not in stop_words]\n",
    "    # instantiate WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lemmatize text with POS\n",
    "    normalized = [lemmatizer.lemmatize(token, pos_tagging(token)) for token in no_stop]\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ed694",
   "metadata": {},
   "source": [
    "<a name='ICBoW'> </a>\n",
    "# Intent Classification with BoW\n",
    "Utilize **word frequency** to construct a measure of the intent of the user's message (**similarity**).\n",
    "\n",
    "Best suited where the **order of words** does not contain much information about the intent of a user's query.\n",
    "\n",
    "## **Process**:\n",
    "#### 1. Preprocess user's query and pre-defined responses \n",
    "\n",
    "* using custom-made function that:\n",
    "    * *Removes punctuation and whitespace*  \n",
    "    * *Lower-case letters*  \n",
    "    * *Tokenize text*  \n",
    "    * *Lemmatize tokens using Part-of-Speech tagging*  \n",
    "\n",
    "    \n",
    "#### 2. Construct a Bag-of-Words dictionary for the processed user's query and responses\n",
    "\n",
    "* ```from collections import Counter``` returns a BoW dictionary\n",
    "\n",
    "#### 3. Count the number of similar words occuring between user's query and each response\n",
    "\n",
    "* using custom-made function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ce8dbb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's input BoW dictionary:\n",
      " Counter({'hello': 1, 'fit': 1, 'elosie': 1, 'dress': 1, 'shoulder': 1, 'broad': 1, 'often': 1, 'size': 1}) \n",
      "\n",
      "Response's a BoW dictionary:\n",
      " Counter({'dress': 1, 'cut': 1, 'polyester': 1, 'blend': 1, 'strechy': 1, 'fit': 1}) \n",
      "\n",
      "Response's b BoW dictionary:\n",
      " Counter({'elosie': 1, 'dress': 1, 'run': 1, 'large': 1, 'suggest': 1, 'take': 1, 'regular': 1, 'size': 1, 'small': 1, 'best': 1, 'fit': 1}) \n",
      "\n",
      "Response's c BoW dictionary:\n",
      " Counter({'elosie': 1, 'dress': 1, 'come': 1, 'green': 1, 'lavender': 1, 'orange': 1}) \n",
      "\n",
      "Number of similar words between user message and\n",
      "\n",
      "response A: 2\n",
      "\n",
      "response B: 4\n",
      "\n",
      "response C: 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "1. Preprocess user's query and pre-defined responses\n",
    "\"\"\"\n",
    "\n",
    "# user's input message\n",
    "user_message = \"Hello! What is the fit of the 'Elosie' dress? My shoulders are broad, so I often size up\" \n",
    "\" for a comfortable fit. Do dress sizes run large or small? Especially in the shoulders?\"\n",
    "\n",
    "# potential responses\n",
    "response_a = \"All of our dresses are cut from a polyester blend for a strechy fit\"\n",
    "response_b = \"The 'Elosie' dress runs large. I suggest you take your regular size or smaller for the best fit.\"\n",
    "response_c = \"The 'Elosie' dress comes in green, lavender, and orange.\"\n",
    "\n",
    "# process both user's input & potential responses\n",
    "user_message_processed = preprocess_text(user_message)\n",
    "response_a_processed = preprocess_text(response_a)\n",
    "response_b_processed = preprocess_text(response_b)\n",
    "response_c_processed = preprocess_text(response_c)\n",
    "\n",
    "\"\"\"\n",
    "2. Construct a Bag-of-Words dictionary for the processed user's query and responses\n",
    "\"\"\"\n",
    "# create and print BoW dictionaries, i.e. {'word': word frequency}, for user's input and potential responses\n",
    "bow_user_message = Counter(user_message_processed)\n",
    "bow_response_a = Counter(response_a_processed)\n",
    "bow_response_b = Counter(response_b_processed)\n",
    "bow_response_c = Counter(response_c_processed)\n",
    "\n",
    "# print BoW dictionaries\n",
    "print(\"User's input BoW dictionary:\\n\", bow_user_message, '\\n')\n",
    "print(\"Response's a BoW dictionary:\\n\", bow_response_a, '\\n')\n",
    "print(\"Response's b BoW dictionary:\\n\", bow_response_b, '\\n')\n",
    "print(\"Response's c BoW dictionary:\\n\", bow_response_c, '\\n')\n",
    "\n",
    "\"\"\"\n",
    "3. Count the number of similar words occuring between user's query and each response\n",
    "\"\"\"\n",
    "\n",
    "def compare_overlap(user_message, possible_response):\n",
    "    \"\"\"Count the similar words between the user_message and potential response.\"\"\"\n",
    "    similar_words = 0\n",
    "    #iterate over tokens in user_message\n",
    "    for token in user_message:\n",
    "        # if token exist in response\n",
    "        if token in possible_response:\n",
    "            # increase similar words by 1\n",
    "            similar_words += 1\n",
    "    # return the number of similar words\n",
    "    return similar_words\n",
    "\n",
    "# print the number of similar words between message and responses\n",
    "print(\"Number of similar words between user message and\\n\")\n",
    "print(\"response A:\", compare_overlap(bow_user_message, bow_response_a))\n",
    "\n",
    "print(\"\\nresponse B:\", compare_overlap(bow_user_message, bow_response_b))\n",
    "\n",
    "print(\"\\nresponse C:\", compare_overlap(bow_user_message, bow_response_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8ecab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shirts', 'clothes', 0.678414398517753]\n",
      "['weekend', 'clothes', 0.2510121169200076]\n",
      "['package', 'clothes', 0.16207362417098703]\n",
      "\n",
      " Hey! I just checked my records, your shipment containing shirts is en route.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load a word2vec model\n",
    "word2vec = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# a list of nouns\n",
    "message_nouns = ['shirts', 'weekend', 'package']\n",
    "\n",
    "# a board category (the blank spot)\n",
    "category = word2vec(\"clothes\")\n",
    "\n",
    "# join words into a single string with a space for seperator\n",
    "tokens = word2vec(\" \".join(message_nouns))\n",
    "\n",
    "def compute_similarity(tokens, category):\n",
    "    \"\"\"Calculate the similarity between a string and a \"blank spot\" word.\"\"\"\n",
    "    output_list = list()\n",
    "    # for each word in a string\n",
    "    for token in tokens:\n",
    "        # print the word, the \"blank spot\" word, and their similarity score\n",
    "        # similarity() defaults to the average of the token vectors\n",
    "        output_list.append([token.text, category.text, token.similarity(category)])\n",
    "    return output_list\n",
    "\n",
    "\n",
    "# print the similarity between each word and \"blank_spot\"\n",
    "for i in range(3):\n",
    "    print(compute_similarity(tokens, category)[i])\n",
    "\n",
    "# assign the word with the highest similarity to the blank_spot, i.e. shirts\n",
    "blank_spot = message_nouns[0]\n",
    "\n",
    "# response to the user\n",
    "bot_response = f\"Hey! I just checked my records, your shipment containing {blank_spot} is en route.\"\n",
    "\"Expect it within the next two days!\"\n",
    "\n",
    "#print bot_response\n",
    "print('\\n',bot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b241c2f",
   "metadata": {},
   "source": [
    "<a name=\"ResponseSelection\"> </a>\n",
    "## Response Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0846abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(input_sentence):\n",
    "    \"\"\"Clean a string.\"\"\"\n",
    "    # lower case letters\n",
    "    input_sentence = input_sentence.lower()\n",
    "    # remove punctuation and whitespace\n",
    "    input_sentence = re.sub(r'[^\\w\\s]','',input_sentence)\n",
    "    # split string into individual words\n",
    "    tokens = word_tokenize(input_sentence)\n",
    "    # remove stopwords\n",
    "    input_sentence = [i for i in tokens if not i in stop_words]\n",
    "    return(input_sentence)\n",
    "\n",
    "  \n",
    "def extract_nouns(tagged_message):\n",
    "    \"\"\"Return a list with just the nouns from a list of words.\"\"\"\n",
    "    message_nouns = list()\n",
    "    for token in tagged_message:\n",
    "        if token[1].startswith(\"N\"):\n",
    "            message_nouns.append(token[0])\n",
    "    return message_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73834a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['morning', 'illinois city', 0.26479153177805814], ['rain', 'illinois city', 0.2857365552501409], ['chicago', 'illinois city', 0.7571821357578838], ['week', 'illinois city', 0.2169059489038729]] \n",
      "\n",
      "Forget about your umbrella; there is no rain forecasted in Chicago this weekend.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Good morning... will it rain in Chicago later this week?\"\n",
    "\n",
    "blank_spot = \"illinois city\"\n",
    "\n",
    "# a selection of responses to match to the blank spot\n",
    "response_a = \"The average temperature this weekend in {} will be 88 degrees. Bring your sunglasses!\"\n",
    "response_b = \"Forget about your umbrella; there is no rain forecasted in {} this weekend.\"\n",
    "response_c = \"This weekend, a warm front from the southeast will keep skies near {} clear.\"\n",
    "\n",
    "responses= [response_a, response_b, response_c]\n",
    "\n",
    "# preprocess documents\n",
    "bow_user_message = Counter(preprocess(user_message))\n",
    "processed_responses = [Counter(preprocess(response)) for response in responses]\n",
    "\n",
    "# build BoW model\n",
    "similarity_list = [compare_overlap(doc, bow_user_message) for doc in processed_responses]\n",
    "\n",
    "# select response with best intent fit\n",
    "response_index = similarity_list.index(max(similarity_list))\n",
    "\n",
    "# extracting entities with word2vec \n",
    "tagged_user_message = pos_tag(preprocess(user_message))\n",
    "message_nouns = extract_nouns(tagged_user_message)\n",
    "\n",
    "# executing word2vec model\n",
    "tokens = word2vec(\" \".join(message_nouns))\n",
    "category = word2vec(blank_spot)\n",
    "word2vec_result = compute_similarity(tokens, category)\n",
    "\n",
    "# select highest scoring entity\n",
    "print(word2vec_result,'\\n')\n",
    "entity = word2vec_result[2][0]\n",
    "\n",
    "# select final response with titlecase\n",
    "final_response = responses[response_index].format(entity.title())\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c818789",
   "metadata": {},
   "source": [
    "<a name=\"ChatBot\"> </a>\n",
    "# Bringing Them All Together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424798b",
   "metadata": {},
   "source": [
    "# Intent Classification with BoW\n",
    "\n",
    ">**The goal is to find the best reponse from a list of pre-defined responses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "09eeb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intent_match(self, responses, user_message):\n",
    "    \"\"\"Select the response that best matches the intent of the user message.\"\"\"\n",
    "\n",
    "    # clean, tokenize, and count term frequency of user_message\n",
    "    bow_user_message = Counter(self.preprocess_text(user_message))\n",
    "    # clean, tokenize potential responses\n",
    "    processed_responses = [Counter(preprocess_text(response)) for response in responses] \n",
    "    # call .compare_overlap method that returns the number of similar \n",
    "    # words b2een bow_user_message & processed_responses\n",
    "    similarity_list = [compare_overlap(doc,  bow_user_message) for doc in processed_responses]\n",
    "    # select the index of the highest similarity score in similarity_list \n",
    "    response_index = similarity_list.index(max(similarity_list))\n",
    "    # return the element at index response_index in responses\n",
    "    return responses[response_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46281e61",
   "metadata": {},
   "source": [
    "# Entity Recognition\n",
    "After determining the best method for the classification of a user’s intent, there is the task of recognizing entities within a user’s message.\n",
    "\n",
    "### 1. Entity Recognition with POS tagging\n",
    "POS tagging is commonly used to identify entities within a user message, as most entities are nouns.\n",
    "\n",
    ">**The goal is to get a list with just the nouns from the user's query.**\n",
    "\n",
    "\n",
    "### 2. Entity Recognition with Word Embeddings\n",
    "While POS tagging extracts key entities in a user message, it does not provide context that allows a chatbot to believably integrate an entity reference into a predefined response.\n",
    "\n",
    "In order to produce a coherent response, the chatbot must **insert entities from a user message** into the blank spots.\n",
    "\n",
    ">**The goal is to get the best noun to fill the blank spot in the bot's response.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b8e599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I help you with? I want to take a telephone\n",
      "telephone\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "import spacy\n",
    "\n",
    "# load word2vec model\n",
    "word2vec = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# get user's query and preprocess it\n",
    "user_message = input('How can I help you with? ')\n",
    "\n",
    "def extract_nouns(tagged_message):\n",
    "    \"\"\"Return a list with just the nouns from a list of words.\"\"\"\n",
    "    message_nouns = []\n",
    "    # for each word in the list of POS-tagged words\n",
    "    for token in tagged_message:\n",
    "        # if the word is tagged as a NOUN\n",
    "        if 'NN' in token[1]:\n",
    "            # add the word at the end of the list\n",
    "            message_nouns.append(token[0])\n",
    "    # return the list of nouns\n",
    "    return message_nouns\n",
    "\n",
    "blank_spot = \"resume\"\n",
    "\n",
    "\n",
    "def find_entities(user_message):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    # preprocess and POS-tag user's query\n",
    "    tagged_user_message = pos_tag(preprocess_text(user_message))\n",
    "    # extract the nouns from user's query\n",
    "    message_nouns = extract_nouns(tagged_user_message)\n",
    "    \"\"\"\n",
    "    execute word2vec model\n",
    "    \"\"\"\n",
    "    # join words into a single string with a space for seperator\n",
    "    tokens = word2vec(\" \".join(message_nouns))\n",
    "    # a broad category\n",
    "    category = word2vec(blank_spot)\n",
    "    # compute the similarity between tokens and category\n",
    "    word2vec_result = compute_similarity(tokens, category)\n",
    "    \n",
    "    word2vec_result.sort(key=lambda x: x[2])\n",
    "    return word2vec_result[-1][0]\n",
    "\n",
    "print(find_entities(user_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd799a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "    \n",
    "\n",
    "class ResumeChatBot:\n",
    "    \"\"\"A representation of a ChatBot intented to serve a resume-style website.\"\"\"\n",
    "    \n",
    "    # define a list of stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # load a word2vec model\n",
    "    word2vec = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    # create possible responses to potential questions\n",
    "    response_a = \"You can find my {} attached to this page as .pdf or .doc format.\"\n",
    "    response_b = \"You can find my past and current project in my GitHub page along with my {}.\"\n",
    "    response_c = \"{} is included in my LinkedIn profile!\"\n",
    "    blank_spot = \"resume\"\n",
    "    \n",
    "    # create a list with all the responses as elements\n",
    "    responses_list = [response_a, response_b, response_c]\n",
    "    \n",
    "    # exit commands so the user can exit the chat\n",
    "    # a response to the question \"Is there anything else that I can help you with?\"\n",
    "    exit_commands = ('quit', 'exit', 'bye', 'goodbye', 'nothing', 'nope', 'no', 'not', )\n",
    "    \n",
    "    \n",
    "    def make_exit(self, user_message):\n",
    "        \"\"\"The user's exit path.\"\"\"\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in user_message:\n",
    "                print(\"Thanks for taking the time to visit my website. I hope to hear back from you soon.\"\n",
    "                       \" Have a great and productive day!\")      \n",
    "        return True\n",
    "\n",
    "    \n",
    "    def chat(self): \n",
    "        \"\"\"Greet user and check if he/she wants to chat.\"\"\"\n",
    "        \n",
    "        # start chat with a Welcoming message\n",
    "        user_message = input(\"Hello there. Welcome to my personal website. How can I help you today? \")\n",
    "        # if the response does not include an exit_command\n",
    "        while not self.make_exit(user_message):\n",
    "            # call .respond method passing the user_message as an argument\n",
    "            user_message = self.respond(user_message)\n",
    "\n",
    "  \n",
    "    def respond(self, user_message):\n",
    "        \"\"\"Find the best reponse based on user's input.\"\"\"\n",
    "        # call .find_intent_match method passing responses and user_message as arguments\n",
    "        best_response = self.find_intent_match(responses_list, user_message)\n",
    "        # call .find_entiries method passsing user_message as an argument\n",
    "        entity = self.find_entities(user_message)\n",
    "        input_message = input(\"Do you need something else? \")\n",
    "        return input_message\n",
    "\n",
    "\n",
    "    def find_intent_match(self, responses_list, user_message):\n",
    "        \"\"\"Select the response that best matches the intent of the user message.\"\"\"\n",
    "        \n",
    "        # clean, tokenize, and count term frequency of user_message\n",
    "        bow_user_message = Counter(self.preprocess(user_message))\n",
    "        # clean, tokenize potential responses\n",
    "        processed_responses = [Counter(preprocess(response)) for response in responses] \n",
    "        # call .compare_overlap method that returns the number of similar \n",
    "        # words b2een bow_user_message & processed_responses\n",
    "        similarity_list = [compare_overlap(doc,  bow_user_message) for doc in processed_responses]\n",
    "        # select the index of the highest similarity score in similarity_list \n",
    "        response_index = similarity_list.index(max(similarity_list))\n",
    "        # return the element at index response_index in responses\n",
    "        return responses[response_index]\n",
    "\n",
    "    \n",
    "    def find_entities(self, user_message):\n",
    "        \"\"\"Select the best word to cover the blank_spot\"\"\"\n",
    "\n",
    "        # preprocess and POS-tag user's query\n",
    "        tagged_user_message = pos_tag(preprocess(user_message))\n",
    "        # extract the nouns from user's query\n",
    "        message_nouns = extract_nouns(tagged_user_message)\n",
    "        \"\"\"\n",
    "        execute word2vec model\n",
    "        \"\"\"\n",
    "        # join words into a single string with a space for seperator\n",
    "        tokens = word2vec(\" \".join(message_nouns))\n",
    "        # a broad category\n",
    "        category = word2vec(blank_spot)\n",
    "        # compute the similarity between tokens and category\n",
    "        word2vec_result = compute_similarity(tokens, category)\n",
    "\n",
    "        word2vec_result.sort(key=lambda x: x[2])\n",
    "        if len(word2vec_result) >= 1:\n",
    "            return word2vec_result[-1][0]\n",
    "        return blank_spot\n",
    "\n",
    "\n",
    "test = ResumeChatBot()\n",
    "test.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
