{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee77992",
   "metadata": {},
   "source": [
    "# Retrieval-Based ChatBot\n",
    "The most popular chatbot implementation in use today!\n",
    "\n",
    "Retrieval-Based ChatBots perform **three main tasks**:\n",
    "\n",
    "1. [**Intent Classification**](#IntentClassification)  \n",
    "Classify the intent of the message from user input.\n",
    "    1. [Intent Classification with Bag-of-Words](#ICBoW)\n",
    "    2. [Intent Classification with Term Frequency-Inverse Document Frequency](#ICTF-IDF)  \n",
    "\n",
    "\n",
    "2. [**Entity Recognition**](#EntityRecognition)  \n",
    "Entities are often the proper nouns of a message.\n",
    "    1. [Entity Recognition with Part-of-Speech tagging](#ERPOS)\n",
    "    2. [Entity Recognition with Word Embeddings](#ERWE)\n",
    "\n",
    "\n",
    "3. [**Response Selection**](#ResponseSelection)  \n",
    "Retrieve the best-fit response from this collection\n",
    "\n",
    "\n",
    "4. [**Working Chatbot Example**](#ChatBot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "754ca518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def pos_tagging(word):\n",
    "    \"\"\"Tag each word with its part of speech.\"\"\"\n",
    "    \n",
    "    # get the already tagged synonyms of the word \n",
    "    probable_pos = wordnet.synsets(word)\n",
    "    # instantiate Counter()\n",
    "    pos_counts = Counter()\n",
    "    \n",
    "    # count the POS of the word's synonyms\n",
    "    pos_counts[\"n\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"n\"])\n",
    "    pos_counts[\"v\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"v\"])\n",
    "    pos_counts[\"a\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"a\"])\n",
    "    pos_counts[\"r\"] = len([synonym for synonym in probable_pos if synonym.pos()==\"r\"])\n",
    "    \n",
    "    # find the most common POS of the word's synonyms\n",
    "    most_likely_pos = pos_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return most_likely_pos\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    1. Strips the text off punctuation.\n",
    "    2. Lower-case letters\n",
    "    3. Splits sentences into tokens\n",
    "    4. Lemmatize tokens\n",
    "    \"\"\"\n",
    "    # strip text off punctuation and lower-case letters\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    # tokenize text\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    # instantiate WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lemmatize text with POS\n",
    "    normalized = [lemmatizer.lemmatize(token, pos_tagging(token)) for token in tokenized]\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813db273",
   "metadata": {},
   "source": [
    "<a name=\"IntentClassification\"> </a>\n",
    "## Intent Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ed694",
   "metadata": {},
   "source": [
    "<a name='ICBoW'> </a>\n",
    "### Option 1: Intent Classification with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce8dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 3, 'be': 2, 'fit': 2, 'dress': 2, 'shoulder': 2, 'size': 2, 'hello': 1, 'what': 1, 'of': 1, 'elosie': 1, 'my': 1, 'broad': 1, 'so': 1, 'i': 1, 'often': 1, 'up': 1, 'for': 1, 'a': 1, 'comfortable': 1, 'do': 1, 'run': 1, 'large': 1, 'or': 1, 'small': 1, 'especially': 1, 'in': 1}) \n",
      "\n",
      "Counter({'a': 2, 'all': 1, 'of': 1, 'our': 1, 'dress': 1, 'sare': 1, 'cut': 1, 'from': 1, 'polyester': 1, 'blend': 1, 'for': 1, 'strechy': 1, 'fit': 1}) \n",
      "\n",
      "Counter({'the': 2, 'elosie': 1, 'dress': 1, 'run': 1, 'large': 1, 'i': 1, 'suggest': 1, 'you': 1, 'take': 1, 'your': 1, 'regular': 1, 'size': 1, 'or': 1, 'small': 1, 'for': 1, 'best': 1, 'fit': 1}) \n",
      "\n",
      "Number of similar words between user message and response A:\n",
      "5\n",
      "\n",
      "Number of similar words between user message and response B:\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "user_message = preprocess_text(\"\"\"Hello! What is the fit of the 'Elosie' dress? \n",
    "                               My shoulders are broad, so I often size up for a comfortable fit. \n",
    "                               Do dress sizes run large or small? Especially in the shoulders?\"\"\")\n",
    "\n",
    "response_a = preprocess_text(\"All of our dresses sare cut from a polyester blend for a strechy fit\")\n",
    "\n",
    "response_b = preprocess_text(\"\"\"The 'Elosie' dress runs large. I suggest you take your regular size or\n",
    "                             smaller for the best fit.\"\"\")\n",
    "\n",
    "\n",
    "# create and print BoW dictionaries ('word': word frequency)\n",
    "bow_user_message = Counter(user_message)\n",
    "print(bow_user_message, '\\n')\n",
    "bow_response_a = Counter(response_a)\n",
    "print(bow_response_a, '\\n')\n",
    "bow_response_b = Counter(response_b)\n",
    "print(bow_response_b, '\\n')\n",
    "\n",
    "def compare_overlap(user_message, possible_response):\n",
    "    \"\"\"Count the similar words between the user_message and potential response.\"\"\"\n",
    "    similar_words = 0\n",
    "    #iterate over tokens in user_message\n",
    "    for token in user_message:\n",
    "        # if token exist in response\n",
    "        if token in possible_response:\n",
    "            # increase similar words by 1\n",
    "            similar_words += 1\n",
    "    # return the number of similar words\n",
    "    return similar_words\n",
    "\n",
    "# print the number of similar words between message and responses\n",
    "print(\"Number of similar words between user message and response A:\")\n",
    "print(compare_overlap(bow_user_message, bow_response_a))\n",
    "print(\"\\nNumber of similar words between user message and response B:\")\n",
    "print(compare_overlap(bow_user_message, bow_response_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2916169",
   "metadata": {},
   "source": [
    "<a name=\"ICTF-IDF\"></a>\n",
    "### Option 2: Intent classification with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f4912ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every dress style is cut from a polyester blend for a strechy fit']\n",
      "['the elosie dress runs large i suggest you take your regular size or smaller']\n",
      "['the elosie dress comes in green lavender and orange']\n",
      "['hello what is the fit of the elosie dress my shoulders are broad so i often size up for a comfortable fit do dress sizes run large or small especially in the shoulders']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# potential responses\n",
    "response_a = \"Every dress style is cut from a polyester blend for a strechy fit.\"\n",
    "response_b = \"The 'Elosie' dress runs large. I suggest you take your regular size or smaller.\"\n",
    "response_c = \"The 'Elosie' dress comes in green, lavender, and orange.\"\n",
    "\n",
    "user_message = \"\"\"Hello! What is the fit of the 'Elosie' dress? \n",
    "                               My shoulders are broad, so I often size up for a comfortable fit. \n",
    "                               Do dress sizes run large or small? Especially in the shoulders?\"\"\"\n",
    "\n",
    "# create a list with the potential responses & user's message\n",
    "documents = [response_a, response_b, response_c, user_message]\n",
    "\n",
    "# create a set with the english stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "def preprocess_text_sent(text):\n",
    "    \"\"\"\n",
    "    1. Strips the text off punctuation.\n",
    "    2. Lower-case letters\n",
    "    3. Tokenize sentences\n",
    "    \"\"\"\n",
    "    # strip text off punctuation and lower-case letters\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    # tokenize text\n",
    "    tokenized = sent_tokenize(cleaned)\n",
    "    # remove stopwords\n",
    "    no_stopwords = [sent for sent in tokenized if sent not in stop_words]\n",
    "#     # instantiate WordNetLemmatizer\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     # lemmatize text with POS\n",
    "#     normalized = [lemmatizer.lemmatize(doc, pos_tagging(doc)) for doc in conc]\n",
    "    \n",
    "    return no_stopwords\n",
    "\n",
    "# create a list with all the documents processed\n",
    "processed_documents = [preprocess_text_sent(doc) for doc in documents]\n",
    "\n",
    "# print processed documents\n",
    "for doc in processed_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ee2dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1 3]]\n",
      "The 'Elosie' dress runs large. I suggest you take your regular size or smaller.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create a list of documents (they are already preprocessed)\n",
    "# the three first elements are potential responses and the last element is the user_message\n",
    "processed_docs = ['every dress style be cut from a polyester blend for a strechy fit', \n",
    "                  'the elosie dress run large i suggest you take your regular size or small',\n",
    "                  'the elosie dress come in green lavender and orange',\n",
    "                  'hello what be the fit of the elosie dress my shoulder be broad so i often'\n",
    "                  'size up for a comfortable fit do dress size run large or small']\n",
    "\n",
    "# instantiate tfidf vectorizer:\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform vectorizer on processed docs\n",
    "tfidf_vectors = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# compute cosine similarity betweeen the user message tf-idf vector and the different response tf-idf vectors\n",
    "cosine_similarities = cosine_similarity(tfidf_vectors[-1], tfidf_vectors)\n",
    "\n",
    "# argsort() is a function from the NumPy package which sorts and array and returns the indices in order\n",
    "# The user_message itself will always have the highest similarity score, and will always be stored at index -1\n",
    "print(cosine_similarities.argsort())\n",
    "\n",
    "# get the index of the most similar response to the user message\n",
    "similar_response_index = cosine_similarities.argsort()[0][-2]\n",
    "\n",
    "best_response = documents[similar_response_index]\n",
    "\n",
    "# print best reponse\n",
    "print(best_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bd52c",
   "metadata": {},
   "source": [
    "<a name=\"EntityRecognition\"></a>\n",
    "## Entity Recognition \n",
    "After determining the best method for the classification of a user’s intent, there is the task of recognizing entities within a user’s message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403443f3",
   "metadata": {},
   "source": [
    "<a name=\"ERPOS\"></a>\n",
    "### Entity Recognition with POS tagging\n",
    "POS tagging is commonly used to identify entities within a user message, as most entities are nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c47f0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'RB'), ('ordered', 'VBD'), ('two', 'CD'), ('t-shirts', 'NNS'), ('this', 'DT'), ('past', 'JJ'), ('weekend', 'NN'), ('when', 'WRB'), ('will', 'MD'), ('my', 'PRP$'), ('package', 'NN'), ('be', 'VB'), ('shipped', 'VBN')] \n",
      "\n",
      "['t-shirts', 'weekend', 'package']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "user_message = [\"i\", \"ordered\", \"two\", \"t-shirts\", \"this\", \"past\",\n",
    "                \"weekend\", \"when\",\"will\", \"my\", \"package\", \"be\", \"shipped\"]\n",
    "\n",
    "# POS-tag each word\n",
    "tagged_user_message = pos_tag(user_message)\n",
    "print(tagged_user_message, '\\n')\n",
    "\n",
    "def extract_nouns(tagged_message):\n",
    "    \"\"\"Return a list with just the nouns from a list of words.\"\"\"\n",
    "    message_nouns = []\n",
    "    # for each word in the list of POS-tagged words\n",
    "    for token in tagged_message:\n",
    "        # if the word is tagged as a NOUN\n",
    "        if 'NN' in token[1]:\n",
    "            # add the word at the end of the list\n",
    "            message_nouns.append(token[0])\n",
    "    # return the list of nouns\n",
    "    return message_nouns\n",
    "\n",
    "# extract the nouns from a list of POS-tagged words\n",
    "user_message_nouns = extract_nouns(tagged_user_message)\n",
    "print(user_message_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c3eb3",
   "metadata": {},
   "source": [
    "<a name=\"ERWE\"></a>\n",
    "### Entity Recognition with Word Embeddings\n",
    "While POS tagging extracts key entities in a user message, it does not provide context that allows a chatbot to believably integrate an entity reference into a predefined response.\n",
    "\n",
    "In order to produce a coherent response, the chatbot must **insert entities from a user message** into the blank spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8ecab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shirts', 'clothes', 0.678414398517753]\n",
      "['weekend', 'clothes', 0.2510121169200076]\n",
      "['package', 'clothes', 0.16207362417098703]\n",
      "\n",
      " Hey! I just checked my records, your shipment containing shirts is en route.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load a word2vec model\n",
    "word2vec = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# a list of nouns\n",
    "message_nouns = ['shirts', 'weekend', 'package']\n",
    "\n",
    "# a board category (the blank spot)\n",
    "category = word2vec(\"clothes\")\n",
    "\n",
    "# join words into a single string with a space for seperator\n",
    "tokens = word2vec(\" \".join(message_nouns))\n",
    "\n",
    "def compute_similarity(tokens, category):\n",
    "    \"\"\"Calculate the similarity between a string and a \"blank spot\" word.\"\"\"\n",
    "    output_list = list()\n",
    "    # for each word in a string\n",
    "    for token in tokens:\n",
    "        # print the word, the \"blank spot\" word, and their similarity score\n",
    "        # similarity() defaults to the average of the token vectors\n",
    "        output_list.append([token.text, category.text, token.similarity(category)])\n",
    "    return output_list\n",
    "\n",
    "# print the similarity between each word and \"blank_spot\"\n",
    "for i in range(3):\n",
    "    print(compute_similarity(tokens, category)[i])\n",
    "\n",
    "# assign the word with the highest similarity to the blank_spot, i.e. shirts\n",
    "blank_spot = message_nouns[0]\n",
    "\n",
    "# response to the user\n",
    "bot_response = f\"Hey! I just checked my records, your shipment containing {blank_spot} is en route.\"\n",
    "\"Expect it within the next two days!\"\n",
    "\n",
    "#print bot_response\n",
    "print('\\n',bot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b241c2f",
   "metadata": {},
   "source": [
    "<a name=\"ResponseSelection\"> </a>\n",
    "## Response Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0846abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(input_sentence):\n",
    "    \"\"\"Clean a string.\"\"\"\n",
    "    # lower case letters\n",
    "    input_sentence = input_sentence.lower()\n",
    "    # remove punctuation and whitespace\n",
    "    input_sentence = re.sub(r'[^\\w\\s]','',input_sentence)\n",
    "    # split string into individual words\n",
    "    tokens = word_tokenize(input_sentence)\n",
    "    # remove stopwords\n",
    "    input_sentence = [i for i in tokens if not i in stop_words]\n",
    "    return(input_sentence)\n",
    "\n",
    "  \n",
    "def extract_nouns(tagged_message):\n",
    "    \"\"\"Return a list with just the nouns from a list of words.\"\"\"\n",
    "    message_nouns = list()\n",
    "    for token in tagged_message:\n",
    "        if token[1].startswith(\"N\"):\n",
    "            message_nouns.append(token[0])\n",
    "    return message_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73834a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['morning', 'illinois city', 0.26479153177805814], ['rain', 'illinois city', 0.2857365552501409], ['chicago', 'illinois city', 0.7571821357578838], ['week', 'illinois city', 0.2169059489038729]] \n",
      "\n",
      "Forget about your umbrella; there is no rain forecasted in Chicago this weekend.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Good morning... will it rain in Chicago later this week?\"\n",
    "\n",
    "blank_spot = \"illinois city\"\n",
    "\n",
    "# a selection of responses to match to the blank spot\n",
    "response_a = \"The average temperature this weekend in {} will be 88 degrees. Bring your sunglasses!\"\n",
    "response_b = \"Forget about your umbrella; there is no rain forecasted in {} this weekend.\"\n",
    "response_c = \"This weekend, a warm front from the southeast will keep skies near {} clear.\"\n",
    "\n",
    "responses= [response_a, response_b, response_c]\n",
    "\n",
    "# preprocess documents\n",
    "bow_user_message = Counter(preprocess(user_message))\n",
    "processed_responses = [Counter(preprocess(response)) for response in responses]\n",
    "\n",
    "# build BoW model\n",
    "similarity_list = [compare_overlap(doc, bow_user_message) for doc in processed_responses]\n",
    "\n",
    "# select response with best intent fit\n",
    "response_index = similarity_list.index(max(similarity_list))\n",
    "\n",
    "# extracting entities with word2vec \n",
    "tagged_user_message = pos_tag(preprocess(user_message))\n",
    "message_nouns = extract_nouns(tagged_user_message)\n",
    "\n",
    "# executing word2vec model\n",
    "tokens = word2vec(\" \".join(message_nouns))\n",
    "category = word2vec(blank_spot)\n",
    "word2vec_result = compute_similarity(tokens, category)\n",
    "\n",
    "# select highest scoring entity\n",
    "print(word2vec_result,'\\n')\n",
    "entity = word2vec_result[2][0]\n",
    "\n",
    "# select final response with titlecase\n",
    "final_response = responses[response_index].format(entity.title())\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c818789",
   "metadata": {},
   "source": [
    "<a name=\"ChatBot\"> </a>\n",
    "## Working ChatBot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb9f69",
   "metadata": {},
   "source": [
    "# Intent classification (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intent_match(self, responses, user_message):\n",
    "    \n",
    "    # call method to give appropriate input to vectorizer\n",
    "    processed_docs = self.process_text(responses, user_message)\n",
    "    \n",
    "    # instantiate tfidf vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit and transform vectorizer on processed docs\n",
    "    tfidf_vectors = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "    # compute cosine similarity betweeen the user message tf-idf vector and the different response tf-idf vectors\n",
    "    cosine_similarities = cosine_similarity(tfidf_vectors[-1], tfidf_vectors)\n",
    "\n",
    "    # argsort() is a function from the NumPy package which sorts and array and returns the indices in order\n",
    "    # The user_message itself will always have the highest similarity score, and will always be stored at index -1\n",
    "    # get the index of the most similar response to the user message\n",
    "    similar_response_index = cosine_similarities.argsort()[0][-2]\n",
    "    \n",
    "    # select the most similar response\n",
    "    best_response = documents[similar_response_index]\n",
    "    \n",
    "    # return the most similar response\n",
    "    return best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8d37ae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm Stratus. Ask me about your local weather!\n",
      "exit\n",
      "The average temperature this weekend in exit will be 88 degrees. Bring your sunglasses!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def find_entities(self, user_message):\n",
    "        tagged_user_message = pos_tag(preprocess(user_message))\n",
    "        message_nouns = extract_nouns(tagged_user_message)\n",
    "\n",
    "        # execute word2vec model\n",
    "        tokens = word2vec(\" \".join(message_nouns))\n",
    "        category = word2vec(blank_spot)\n",
    "        word2vec_result = compute_similarity(tokens, category)\n",
    "        word2vec_result.sort(key=lambda x: x[2])\n",
    "        return word2vec_result[-1][0]\n",
    "\n",
    "    \n",
    "    def respond(self, user_message):\n",
    "        best_response = self.find_intent_match(responses, user_message)\n",
    "        entity = self.find_entities(user_message)\n",
    "        print(best_response.format(entity))\n",
    "\n",
    "        \n",
    "    def chat(self):\n",
    "        user_message = input(\"Hi, I'm Stratus. Ask me about your local weather!\\n\")\n",
    "        self.respond(user_message)\n",
    "\n",
    "# create ChatBot() instance:\n",
    "chatbot = ChatBot()\n",
    "# call .chat() method:\n",
    "chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a8d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "    \n",
    "\n",
    "class ResumeChatBot:\n",
    "    \"\"\"A representation of a ChatBot intented to serve a resume-style website.\"\"\"\n",
    "\n",
    "    # create possible responses to potential questions\n",
    "    response_a = \"You can find my {} attached to this page as .pdf or .doc format.\"\n",
    "    response_b = \"You can find my past and current project in my GitHub page along with my {}.\"\n",
    "    response_c = \"{} is included in my LinkedIn profile!\"\n",
    "    blank_spot = \"resume\"\n",
    "    # create a list with all the responses as elements\n",
    "    responses = [response_a, response_b, response_c]\n",
    "    \n",
    "    # exit commands so the user can exit the chat\n",
    "    # a response to the question \"Is there anything else that I can help you with?\"\n",
    "    exit_commands = ('quit', 'exit', 'bye', 'goodbye', 'nothing', 'nope', 'no', 'not', )\n",
    "    \n",
    "    \n",
    "    # load a word2vec model\n",
    "    word2vec = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    def make_exit(self, user_message):\n",
    "        \"\"\"The user's exit path.\"\"\"\n",
    "        \n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in user_message:\n",
    "                print(\"Thanks for taking the time to visit my website. I hope to hear back from you soon.\"\n",
    "                       \" Have a great and productive day!\")      \n",
    "        return True\n",
    "\n",
    "    \n",
    "    def chat(self): \n",
    "        \"\"\"Greet user and check if he/she wants to chat.\"\"\"\n",
    "        \n",
    "        # start chat with a Welcoming message\n",
    "        user_message = input(\"Hello there. Welcome to my personal website. How can I help you today? \")\n",
    "        # if the response does not include an exit_command\n",
    "        while not self.make_exit(user_message):\n",
    "            # call .respond method passing the user_message as an argument\n",
    "            user_message = self.respond(user_message)\n",
    "\n",
    "  \n",
    "    def respond(self, user_message):\n",
    "        \"\"\"Find the best reponse based on user's input.\"\"\"\n",
    "        # call .find_intend_match method passing responses and user_message as arguments\n",
    "        best_response = self.find_intent_match(responses, user_message)\n",
    "        entity = self.find_entities(user_message)\n",
    "        print(best_response.format(entity))\n",
    "        input_message = input(\"Do you need something else? \")\n",
    "        return input_message\n",
    "    \n",
    "\n",
    "    def find_intent_match(self, responses, user_message):\n",
    "        \"\"\"Select the response that best matches the intent of the user message.\"\"\"\n",
    "        \n",
    "        # clean, tokenize, and count term frequency of user_message\n",
    "        bow_user_message = Counter(self.preprocess_text(user_message))\n",
    "        # clean, tokenize potential responses\n",
    "        processed_responses = [Counter(preprocess_text(response)) for response in responses] \n",
    "        # call .compare_overlap method that returns the number of similar \n",
    "        # words b2een bow_user_message & processed_responses\n",
    "        similarity_list = [compare_overlap(doc,  bow_user_message) for doc in processed_responses]\n",
    "        # select the index of the highest similarity score in similarity_list \n",
    "        response_index = similarity_list.index(max(similarity_list))\n",
    "        # return the element at index response_index in responses\n",
    "        return responses[response_index]\n",
    "\n",
    "\n",
    "    def find_entities(self, user_message):\n",
    "        tagged_user_message = pos_tag(preprocess(user_message))\n",
    "        message_nouns = extract_nouns(tagged_user_message)\n",
    "        # create a concatenated string from message_nouns\n",
    "        tokens = word2vec(\" \".join(message_nouns))\n",
    "        category = word2vec(blank_spot)\n",
    "        word2vec_result = compute_similarity(tokens, category)\n",
    "        word2vec_result.sort(key= lambda x: x[2])\n",
    "        if len(word2vec_result) >= 1:\n",
    "            return word2vec_result[-1][0]\n",
    "        return blank_spot\n",
    "\n",
    "  \n",
    "    def extract_nouns(tagged_message):\n",
    "        message_nouns = list()\n",
    "        for token in tagged_message:\n",
    "            if token[1].startswith(\"N\"):\n",
    "                message_nouns.append(token[0])\n",
    "        return message_nouns\n",
    "\n",
    "    def compute_similarity(tokens, category):\n",
    "        output_list = list()\n",
    "        for token in tokens:\n",
    "            output_list.append([token.text, category.text, token.similarity(category)])\n",
    "        return output_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e554b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there. Welcome to my personal website. How can I help you today? exit\n",
      "Thanks for taking the time to visit my website. I hope to hear back from you soon.Have a great and productive day!\n"
     ]
    }
   ],
   "source": [
    "testbot = ResumeChatBot()\n",
    "testbot.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
