{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842f6cad",
   "metadata": {},
   "source": [
    "# Deep Learning & Generative ChatBots\n",
    "By using **neural networks** with many hidden layers — known as **deep learning** —, generative chatbot models **can build sentences that are completely original** rather than retrieved from a list of possible responses.\n",
    "\n",
    "1. [**Long Short-Term Memory (LSTM) networks**](#LSTMs)\n",
    "2. [**seq2seq**](#seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ab878",
   "metadata": {},
   "source": [
    "<a name='LSTMs'></a>\n",
    "## Long Short-Term Memory (LSTM) networks\n",
    "**Recurrent Neural Networks (RNNs)** are specifically designed to process inputs in a **temporal order** and update future based on the past (speech recognition, machine translation). **LSTMs** are a special type of RNNs that can generate language that is both **persistent** across interactions, and **adaptable** to new conversations.\n",
    " \n",
    "The chain structure of RNNs places them in close relation to **data with a clear temporal ordering** or list-like structure — such as **human language**, where words obviously appear one after another. **Standard RNNs** are certainly the best fit for tasks that involve sequences, like the translation of a sentence from one language to another. As the **gap between context and the word to predict** grows, standard RNNs become less and less accurate (**long-term dependency problem**).\n",
    "\n",
    "The most important aspect of an LSTM is the way in which the transformed input data is combined by adding results to **state** (**cell memory**), represented as vectors. There are two states that are produced for the first step in the sequence and then carried over as subsequent inputs are processed: cell state, and hidden state.\n",
    "\n",
    "The **cell state** carries information through the network as we process a sequence of inputs. At each timestep, or step in the sequence, the updated input is appended to the cell state by a gate, which controls how much of the input should be included in **the final product of the cell state**. This final product, which is fed as input to the next neural network layer at the next timestep, is called a **hidden state**. The final output of a neural network is often the result contained in the final hidden state, or an average of the results across all hidden states in the network.\n",
    "\n",
    "The persistence of the majority of a cell state across data transformations, combined with incremental additions controlled by the gates, allows for important **information from the initial input data to be maintained** in the neural network. Ultimately, this allows for information from far earlier in the input data to be used in decisions at any point in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97644648",
   "metadata": {},
   "source": [
    "<a name=\"seq2seq\"> </a>\n",
    "## Sequence-to-Sequence (seq2seq)\n",
    "One of the most **common neural models used for text generation** is the seq2seq model. A type of **encoder-decoder model**, which uses RNNs like LSTM in order to generate output, token by token or character by character.\n",
    "\n",
    "Used for machine translation, text summary generation, chatbots, Named Entity Recognition (NER), and speech recognition.\n",
    "\n",
    "seq2seq networks have two parts:\n",
    "1. An **encoder** that accepts language (or audio or video) input. The output matrix of the encoder is discarded, but its state is preserved as a vector.\n",
    "2. A **decoder** that takes the encoder’s final state (or memory) as its initial state. By using a technique called “teacher forcing” to train the decoder to predict the following text (characters or words) in a target sequence given the previous text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eee269",
   "metadata": {},
   "source": [
    "## Building a pretty limited **English-to-Spanish translator**. \n",
    "There are a few **neural network libraries** such as **TensorFlow with the Keras API**.\n",
    "\n",
    "We’ll need the following for our Keras implementation:\n",
    "\n",
    "1. **Vocabulary sets** for both our input (English) and target (Spanish) data\n",
    "2. The **total number of unique word tokens** we have for each set\n",
    "3. The **maximum sentence length** we’re using for each language\n",
    "\n",
    "We also need to **mark the start and end of each document** (sentence) in the target samples so that the model recognizes where to begin and end its text generation. \n",
    "\n",
    "One way to do this is adding ```<START>``` at the beginning and ```<END>``` at the end of each target document (in this case, this will be our Spanish sentences). \n",
    "\n",
    "For example, ```Estoy feliz.``` becomes ```<START> Estoy feliz. <END>```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b495c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"We'll see.\\tDespués veremos.\", \"We'll see.\\tYa veremos.\", \"We'll try.\\tLo intentaremos.\", \"We've won!\\t¡Hemos ganado!\", 'Well done.\\tBien hecho.', \"What's up?\\t¿Qué hay?\", 'Who cares?\\t¿A quién le importa?', 'Who drove?\\t¿Quién condujo?', 'Who drove?\\t¿Quién conducía?', 'Who is he?\\t¿Quién es él?', 'Who is it?\\t¿Quién es?']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import re\n",
    "\n",
    "# Importing our translations\n",
    "data_path = \"engspan.txt\"\n",
    "\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67abeb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll\n",
      "see\n",
      ".\n",
      "<START>\n",
      "Después\n",
      "veremos\n",
      ".\n",
      "<END>\n",
      "We'll\n",
      "see\n",
      ".\n",
      "<START>\n",
      "Ya\n",
      "veremos\n",
      ".\n",
      "<END>\n",
      "We'll\n",
      "try\n",
      ".\n",
      "<START>\n",
      "Lo\n",
      "intentaremos\n",
      ".\n",
      "<END>\n",
      "We've\n",
      "won\n",
      "!\n",
      "<START>\n",
      "¡\n",
      "Hemos\n",
      "ganado\n",
      "!\n",
      "<END>\n",
      "Well\n",
      "done\n",
      ".\n",
      "<START>\n",
      "Bien\n",
      "hecho\n",
      ".\n",
      "<END>\n",
      "What's\n",
      "up\n",
      "?\n",
      "<START>\n",
      "¿\n",
      "Qué\n",
      "hay\n",
      "?\n",
      "<END>\n",
      "Who\n",
      "cares\n",
      "?\n",
      "<START>\n",
      "¿\n",
      "A\n",
      "quién\n",
      "le\n",
      "importa\n",
      "?\n",
      "<END>\n",
      "Who\n",
      "drove\n",
      "?\n",
      "<START>\n",
      "¿\n",
      "Quién\n",
      "condujo\n",
      "?\n",
      "<END>\n",
      "Who\n",
      "drove\n",
      "?\n",
      "<START>\n",
      "¿\n",
      "Quién\n",
      "conducía\n",
      "?\n",
      "<END>\n",
      "Who\n",
      "is\n",
      "he\n",
      "?\n",
      "<START>\n",
      "¿\n",
      "Quién\n",
      "es\n",
      "él\n",
      "?\n",
      "<END>\n",
      "Who\n",
      "is\n",
      "it\n",
      "?\n",
      "<START>\n",
      "¿\n",
      "Quién\n",
      "es\n",
      "?\n",
      "<END>\n"
     ]
    }
   ],
   "source": [
    "# Building empty lists to hold sentences\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "\n",
    "# Building empty vocabulary sets\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "for line in lines:\n",
    "    # Input and target sentences are separated by tabs\n",
    "    input_doc, target_doc = line.split('\\t')\n",
    "    # Appending each input sentence to input_docs\n",
    "    input_docs.append(input_doc)\n",
    "    # Splitting words from punctuation\n",
    "    target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "    # Redefine target_doc and append it to target_docs\n",
    "    target_doc = '<START> ' + target_doc + ' <END>'\n",
    "    target_docs.append(target_doc)\n",
    "\n",
    "    # split up each sentence into words and add each unique word to the vocabulary set\n",
    "    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "        print(token)\n",
    "        if token not in input_tokens:\n",
    "          input_tokens.add(token)\n",
    "    for token in target_doc.split():\n",
    "        print(token)\n",
    "        if token not in target_tokens:\n",
    "            target_tokens.add(token)\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "# Create num_encoder_tokens and num_decoder_tokens\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "try:\n",
    "  max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "  max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "except ValueError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf0fc3",
   "metadata": {},
   "source": [
    "### Training Setup (part 1)\n",
    "For each sentence, **Keras expects a NumPy matrix containing one-hot vectors** for each token.\n",
    "\n",
    "In order to **vectorize our data** and later **translate it from vectors** we need:\n",
    "1. Features dictionary for English\n",
    "2. Features dictionary for Spanish\n",
    "3. Reverse features dictionary for English (where the keys and values are swapped)\n",
    "4. Reverse features dictionary for Spanish  \n",
    "\n",
    "Once we have all four we will vectorize the data. We will need vectors to input into our encoder and decoder, as well as a vector of target data we can use to train the decoder.\n",
    "\n",
    "Because each matrix is almost all zeros, we’ll use ```numpy.zeros()``` from the NumPy library to build them out:  \n",
    "\n",
    "```encoder_input_data = np.zeros((len(input_docs), max_encoder_seq_length, num_encoder_tokens), dtype='float32')```\n",
    "\n",
    "We defined a NumPy matrix of zeros called encoder_input_data with two arguments:\n",
    "1. The **shape of the matrix** — in our case the number of documents (or sentences) by the maximum token sequence length (the longest sentence we want to see) by the number of unique tokens (or words)\n",
    "2. The **data type** we want — in our case NumPy’s float32, which can speed up the processing a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce695f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 11\n",
      "Number of unique input tokens: 18\n",
      "Number of unique output tokens: 27\n",
      "Max sequence length for inputs: 4\n",
      "Max sequence length for outputs: 12\n",
      "\n",
      "Here's the first item in the encoder input matrix:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      "The number of columns should match the number of unique input tokens and the number of rows should match the maximum sequence length for input sentences.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Number of samples:', len(input_docs))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# build input features dictionary\n",
    "input_features_dict = dict([(token, i) for i, token in enumerate(input_tokens)])\n",
    "# build target features_dictionary\n",
    "target_features_dict = dict([(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "# reverse-lookup token index to decode sequences back to something readable\n",
    "# build reverse input features dictionary\n",
    "reverse_input_features_dict = dict((i, token) for token, i in input_features_dict.items())\n",
    "# same for reverse target features dictionary\n",
    "reverse_target_features_dict = dict((i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "# build a Numpy matrix of zeros\n",
    "encoder_input_data = np.zeros((len(input_docs), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "print(\"\\nHere's the first item in the encoder input matrix:\\n\", encoder_input_data[0], \"\\n\\nThe number of columns should match the number of unique input tokens and the number of rows should match the maximum sequence length for input sentences.\")\n",
    "\n",
    "# build out the decoder_input_data matrix\n",
    "decoder_input_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "# build out the decoder_target_data matrix\n",
    "decoder_target_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e4c55",
   "metadata": {},
   "source": [
    "### Training Setup (part 2)\n",
    "At this point we need to fill out the 1s in each vector. We can loop over each English-Spanish pair in our training sample using the features dictionaries to add a 1 for the token in question.\n",
    "\n",
    "You’ll notice the vectors have timesteps — we use these to track where in a given document (sentence) we are.\n",
    "\n",
    "To build out a three-dimensional NumPy matrix of one-hot vectors, we can assign a value of 1 for a given word at a given timestep in a given line:\n",
    "\n",
    "```matrix_name[line, timestep, features_dict[token]] = 1.```\n",
    "\n",
    "Keras will fit — or train — the seq2seq model using these matrices of one-hot vectors:\n",
    "* the encoder input data\n",
    "* the decoder input data\n",
    "* the decoder target data\n",
    "\n",
    "Hang on a second, why build two matrices of decoder data? Aren’t we just encoding and decoding?\n",
    "\n",
    "The reason has to do with a technique known as **teacher forcing** that most seq2seq models employ during training. Here’s the idea: we have a Spanish input token from the previous timestep to help train the model for the current timestep’s target token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a55617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input timestep & token: 0 We'll\n",
      "3\n",
      "Encoder input timestep & token: 1 see\n",
      "14\n",
      "Encoder input timestep & token: 2 .\n",
      "1\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 Después\n",
      "Decoder input timestep & token: 2 veremos\n",
      "Decoder input timestep & token: 3 .\n",
      "Decoder input timestep & token: 4 <END>\n",
      "Decoder target timestep: 4\n",
      "Encoder input timestep & token: 0 We'll\n",
      "3\n",
      "Encoder input timestep & token: 1 see\n",
      "14\n",
      "Encoder input timestep & token: 2 .\n",
      "1\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 Ya\n",
      "Decoder input timestep & token: 2 veremos\n",
      "Decoder input timestep & token: 3 .\n",
      "Decoder input timestep & token: 4 <END>\n",
      "Decoder target timestep: 4\n",
      "Encoder input timestep & token: 0 We'll\n",
      "3\n",
      "Encoder input timestep & token: 1 try\n",
      "15\n",
      "Encoder input timestep & token: 2 .\n",
      "1\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 Lo\n",
      "Decoder input timestep & token: 2 intentaremos\n",
      "Decoder input timestep & token: 3 .\n",
      "Decoder input timestep & token: 4 <END>\n",
      "Decoder target timestep: 4\n",
      "Encoder input timestep & token: 0 We've\n",
      "4\n",
      "Encoder input timestep & token: 1 won\n",
      "17\n",
      "Encoder input timestep & token: 2 !\n",
      "0\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 ¡\n",
      "Decoder input timestep & token: 2 Hemos\n",
      "Decoder input timestep & token: 3 ganado\n",
      "Decoder input timestep & token: 4 !\n",
      "Decoder input timestep & token: 5 <END>\n",
      "Decoder target timestep: 5\n",
      "Encoder input timestep & token: 0 Well\n",
      "5\n",
      "Encoder input timestep & token: 1 done\n",
      "9\n",
      "Encoder input timestep & token: 2 .\n",
      "1\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 Bien\n",
      "Decoder input timestep & token: 2 hecho\n",
      "Decoder input timestep & token: 3 .\n",
      "Decoder input timestep & token: 4 <END>\n",
      "Decoder target timestep: 4\n",
      "Encoder input timestep & token: 0 What's\n",
      "6\n",
      "Encoder input timestep & token: 1 up\n",
      "16\n",
      "Encoder input timestep & token: 2 ?\n",
      "2\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 ¿\n",
      "Decoder input timestep & token: 2 Qué\n",
      "Decoder input timestep & token: 3 hay\n",
      "Decoder input timestep & token: 4 ?\n",
      "Decoder input timestep & token: 5 <END>\n",
      "Decoder target timestep: 5\n",
      "Encoder input timestep & token: 0 Who\n",
      "7\n",
      "Encoder input timestep & token: 1 cares\n",
      "8\n",
      "Encoder input timestep & token: 2 ?\n",
      "2\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 ¿\n",
      "Decoder input timestep & token: 2 A\n",
      "Decoder input timestep & token: 3 quién\n",
      "Decoder input timestep & token: 4 le\n",
      "Decoder input timestep & token: 5 importa\n",
      "Decoder input timestep & token: 6 ?\n",
      "Decoder input timestep & token: 7 <END>\n",
      "Decoder target timestep: 7\n",
      "Encoder input timestep & token: 0 Who\n",
      "7\n",
      "Encoder input timestep & token: 1 drove\n",
      "10\n",
      "Encoder input timestep & token: 2 ?\n",
      "2\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 ¿\n",
      "Decoder input timestep & token: 2 Quién\n",
      "Decoder input timestep & token: 3 condujo\n",
      "Decoder input timestep & token: 4 ?\n",
      "Decoder input timestep & token: 5 <END>\n",
      "Decoder target timestep: 5\n",
      "Encoder input timestep & token: 0 Who\n",
      "7\n",
      "Encoder input timestep & token: 1 drove\n",
      "10\n",
      "Encoder input timestep & token: 2 ?\n",
      "2\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 ¿\n",
      "Decoder input timestep & token: 2 Quién\n",
      "Decoder input timestep & token: 3 conducía\n",
      "Decoder input timestep & token: 4 ?\n",
      "Decoder input timestep & token: 5 <END>\n",
      "Decoder target timestep: 5\n",
      "Encoder input timestep & token: 0 Who\n",
      "7\n",
      "Encoder input timestep & token: 1 is\n",
      "12\n",
      "Encoder input timestep & token: 2 he\n",
      "11\n",
      "Encoder input timestep & token: 3 ?\n",
      "2\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 ¿\n",
      "Decoder input timestep & token: 2 Quién\n",
      "Decoder input timestep & token: 3 es\n",
      "Decoder input timestep & token: 4 él\n",
      "Decoder input timestep & token: 5 ?\n",
      "Decoder input timestep & token: 6 <END>\n",
      "Decoder target timestep: 6\n",
      "Encoder input timestep & token: 0 Who\n",
      "7\n",
      "Encoder input timestep & token: 1 is\n",
      "12\n",
      "Encoder input timestep & token: 2 it\n",
      "13\n",
      "Encoder input timestep & token: 3 ?\n",
      "2\n",
      "Decoder input timestep & token: 0 <START>\n",
      "Decoder input timestep & token: 1 ¿\n",
      "Decoder input timestep & token: 2 Quién\n",
      "Decoder input timestep & token: 3 es\n",
      "Decoder input timestep & token: 4 ?\n",
      "Decoder input timestep & token: 5 <END>\n",
      "Decoder target timestep: 5\n"
     ]
    }
   ],
   "source": [
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "\n",
    "    for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "\n",
    "        print(\"Encoder input timestep & token:\", timestep, token)\n",
    "        print(input_features_dict[token])\n",
    "        # Assign 1. for the current line, timestep, & word in encoder_input_data:\n",
    "        encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "\n",
    "    for timestep, token in enumerate(target_doc.split()):\n",
    "\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        print(\"Decoder input timestep & token:\", timestep, token)\n",
    "        # Assign 1. for the current line, timestep, & word in decoder_input_data:\n",
    "        decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "    \n",
    "    if timestep > 0:\n",
    "        # decoder_target_data is ahead by 1 timestep and doesn't include the start token.\n",
    "        print(\"Decoder target timestep:\", timestep)\n",
    "    \n",
    "        # Assign 1. for the current line, timestep, & word in decoder_target_data\n",
    "    if timestep > 0:\n",
    "        decoder_target_data[line, timestep-1, target_features_dict[token]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6a090",
   "metadata": {},
   "source": [
    "### Encoder Training Setup\n",
    "\n",
    "Deep learning models in Keras are built in layers, where each layer is a step in the model.\n",
    "\n",
    "Our encoder requires two layer types from Keras:\n",
    "1. An **input layer**, which defines a matrix to hold all the one-hot vectors that we’ll feed to the model.\n",
    "2. An **LSTM layer**, with some output dimensionality.\n",
    "\n",
    "Next, we **set up the input layer**, which requires some number of dimensions that we’re providing. \n",
    "\n",
    "In this case, we know that we’re passing in all the encoder tokens, but we don’t necessarily know our batch size (how many sentences we’re feeding the model at a time). Fortunately, we can say None because the code is written to handle varying batch sizes, so we don’t need to specify that dimension.\n",
    "\n",
    "For the **LSTM layer** we need to **select the dimensionality**(the size of the LSTM’s hidden states, which helps determine how closely the model molds itself to the training data) and whether to return the state (in this case we do):\n",
    "\n",
    "**The only thing we want from the encoder is its final states.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e23a54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "# create the input layer\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# create the LSTM layer:\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "\n",
    "# retrieve the outputs and states:\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# put the states together in a list:\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b65b8a",
   "metadata": {},
   "source": [
    "### Decoder Training Setup\n",
    "The decoder looks a lot like the encoder, with an input layer and an LSTM layer that we use together.\n",
    "\n",
    "\n",
    "This time **we care about full return sequences**. However, with our decoder, we pass in the state data from the encoder, along with the decoder inputs. This time, we will keep the output instead of the states.\n",
    "\n",
    "We also need to run the output through a final activation layer, using the **Softmax function**, that will give us the probability distribution for each token. The final layer also transforms our LSTM output from a dimensionality of whatever we gave it (in our case, 10) to the number of unique words within the hidden layer’s vocabulary (i.e., the number of unique target tokens, which is definitely more than 10).\n",
    " \n",
    "Keras’s implementation could work with several layer types, but **Dense is the least complex**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3877b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# the decoder input and LSTM layers\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# this time we care about full return sequences\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "\n",
    "# retrieve the LSTM outputs and states\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# build a final Dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# filter outputs through the Dense layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07efff",
   "metadata": {},
   "source": [
    "<a name=\"buildseq2seq\"> </a>\n",
    "### Build & Train seq2seq\n",
    "\n",
    "1. **Define the seq2seq model** using the ```Model()``` function from Keras. To make it a seq2seq model, feed it the encoder and decoder inputs, as well as the decoder output:\n",
    "\n",
    "```model = Model([encoder_inputs, decoder_inputs], decoder_outputs)```\n",
    "\n",
    "2. **Train** the model. \n",
    "    1. Compile everything. Keras models demand two arguments to compile:\n",
    "    \n",
    "    \n",
    "* An ```optimizer``` (RMSprop is a fancy version of gradient descent) to help minimize our error rate (how bad the model is  at guessing the true next word given the previous words in a sentence).  \n",
    "* A ```loss``` function (logarithm-based cross-entropy function) to determine the error rate.\n",
    "        \n",
    "Add **accuracy** to pay attention to while training.\n",
    "\n",
    "```model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])```\n",
    "\n",
    "3. **Fit** the compiled model to both the encoder and decoder input data (what we pass into the model), the decoder target data (what we expect the model to return given the data we passed in), and some numbers we can adjust as needed:\n",
    "\n",
    "\n",
    "* ```batch_size``` (smaller batch sizes mean more time, and for some problems, smaller batch sizes will be better, while for other problems, larger batch sizes are better)\n",
    "* Number of ```epochs``` or cycles of training (more epochs mean a model that is more trained on the dataset, and that the process will take more time)\n",
    "* ```validation_split``` (what percentage of the data should be set aside for validating — and determining when to stop training your model — rather than training)\n",
    "    \n",
    "```model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=10, epochs=100, validation_split=0.2)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0b93e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, None, 18)]   0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, None, 27)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  [(None, 256),        281600      ['input_7[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, None, 256),  290816      ['input_8[0][0]',                \n",
      "                                 (None, 256),                     'lstm_6[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_6[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 27)     6939        ['lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 579,355\n",
      "Trainable params: 579,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Training the model:\n",
      "\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.2748 - accuracy: 0.0000e+00 - val_loss: 0.2633 - val_accuracy: 0.0833\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2642 - accuracy: 0.0833 - val_loss: 0.2493 - val_accuracy: 0.0833\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2495 - accuracy: 0.0833 - val_loss: 0.2066 - val_accuracy: 0.0833\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2069 - accuracy: 0.0833 - val_loss: 0.1607 - val_accuracy: 0.0833\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1626 - accuracy: 0.0833 - val_loss: 0.1442 - val_accuracy: 0.0833\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1436 - accuracy: 0.0833 - val_loss: 0.0755 - val_accuracy: 0.0833\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0733 - accuracy: 0.0833 - val_loss: 0.1529 - val_accuracy: 0.0833\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1410 - accuracy: 0.0833 - val_loss: 0.0969 - val_accuracy: 0.0833\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0911 - accuracy: 0.0833 - val_loss: 0.1267 - val_accuracy: 0.0833\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1201 - accuracy: 0.0833 - val_loss: 0.1059 - val_accuracy: 0.0833\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1016 - accuracy: 0.0833 - val_loss: 0.1181 - val_accuracy: 0.0833\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1143 - accuracy: 0.0833 - val_loss: 0.1090 - val_accuracy: 0.0833\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1062 - accuracy: 0.0833 - val_loss: 0.1148 - val_accuracy: 0.0833\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1125 - accuracy: 0.0833 - val_loss: 0.1098 - val_accuracy: 0.0833\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1081 - accuracy: 0.0833 - val_loss: 0.1110 - val_accuracy: 0.0833\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1096 - accuracy: 0.0833 - val_loss: 0.1097 - val_accuracy: 0.0833\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1084 - accuracy: 0.0833 - val_loss: 0.1104 - val_accuracy: 0.0833\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1093 - accuracy: 0.0833 - val_loss: 0.1090 - val_accuracy: 0.0833\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1082 - accuracy: 0.0833 - val_loss: 0.1082 - val_accuracy: 0.0833\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1075 - accuracy: 0.0833 - val_loss: 0.1078 - val_accuracy: 0.0833\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1071 - accuracy: 0.0833 - val_loss: 0.1077 - val_accuracy: 0.0833\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1071 - accuracy: 0.0833 - val_loss: 0.1075 - val_accuracy: 0.0833\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1070 - accuracy: 0.0833 - val_loss: 0.1072 - val_accuracy: 0.0833\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1067 - accuracy: 0.0833 - val_loss: 0.1069 - val_accuracy: 0.0833\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1063 - accuracy: 0.0833 - val_loss: 0.1064 - val_accuracy: 0.0833\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1060 - accuracy: 0.0833 - val_loss: 0.1060 - val_accuracy: 0.0833\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1055 - accuracy: 0.0833 - val_loss: 0.1055 - val_accuracy: 0.0833\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1051 - accuracy: 0.0833 - val_loss: 0.1052 - val_accuracy: 0.0833\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1048 - accuracy: 0.0833 - val_loss: 0.1050 - val_accuracy: 0.0833\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1047 - accuracy: 0.0833 - val_loss: 0.1049 - val_accuracy: 0.0833\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1046 - accuracy: 0.0833 - val_loss: 0.1049 - val_accuracy: 0.0833\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1046 - accuracy: 0.0833 - val_loss: 0.1048 - val_accuracy: 0.0833\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1045 - accuracy: 0.0833 - val_loss: 0.1045 - val_accuracy: 0.0833\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1043 - accuracy: 0.0833 - val_loss: 0.1041 - val_accuracy: 0.0833\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1038 - accuracy: 0.0833 - val_loss: 0.1032 - val_accuracy: 0.0833\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1030 - accuracy: 0.0833 - val_loss: 0.1018 - val_accuracy: 0.0833\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1016 - accuracy: 0.0833 - val_loss: 0.1003 - val_accuracy: 0.0833\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1001 - accuracy: 0.0833 - val_loss: 0.0993 - val_accuracy: 0.0833\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0991 - accuracy: 0.0833 - val_loss: 0.0988 - val_accuracy: 0.0833\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0987 - accuracy: 0.0833 - val_loss: 0.0989 - val_accuracy: 0.0833\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0988 - accuracy: 0.0833 - val_loss: 0.0992 - val_accuracy: 0.0833\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0991 - accuracy: 0.0833 - val_loss: 0.0995 - val_accuracy: 0.0833\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0994 - accuracy: 0.0833 - val_loss: 0.0998 - val_accuracy: 0.0833\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0997 - accuracy: 0.0833 - val_loss: 0.1000 - val_accuracy: 0.0833\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1000 - accuracy: 0.0833 - val_loss: 0.1002 - val_accuracy: 0.0833\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1002 - accuracy: 0.0833 - val_loss: 0.1003 - val_accuracy: 0.0833\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1003 - accuracy: 0.0833 - val_loss: 0.1004 - val_accuracy: 0.0833\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1004 - accuracy: 0.0833 - val_loss: 0.1005 - val_accuracy: 0.0833\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1004 - accuracy: 0.0833 - val_loss: 0.1005 - val_accuracy: 0.0833\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1005 - accuracy: 0.0833 - val_loss: 0.1005 - val_accuracy: 0.0833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2303dfaa8e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Decoder training setup:\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# building the training model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "print(\"Model summary:\\n\")\n",
    "training_model.summary()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# compile the model\n",
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# batch size and number of epochs\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "print(\"Training the model:\\n\")\n",
    "# train the model:\n",
    "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                   batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f98ec",
   "metadata": {},
   "source": [
    "<a name=\"Testing\"> </a>\n",
    "### Setup for Testing\n",
    "To generate some original output text, the seq2seq model  architecture needs to be **redefined in pieces**.\n",
    "\n",
    "**The model used for training only works when we already know the target sequence**. This time, we have no idea what the Spanish should be for the English we pass in! So we need a model that will **decode step-by-step** instead of using teacher forcing.\n",
    "\n",
    "1. Build an encoder model with our encoder inputs and the placeholders for the encoder’s output states:\n",
    "\n",
    "```encoder_model = Model(encoder_inputs, encoder_states)```\n",
    "\n",
    "2. We need placeholders for the decoder’s input states, which we can build as input layers and store together. We don’t know what we want to decode yet or what hidden state we’re going to end up with, so we need to do everything step-by-step. We need to pass the encoder’s final hidden state to the decoder, sample a token, and get the updated hidden state back. Then we’ll be able to (manually) pass the updated hidden state back into the network:\n",
    "\n",
    "```\n",
    "latent_dim = 256\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "```\n",
    "\n",
    "3. Using the decoder LSTM and decoder dense layer (with the activation function) that we trained earlier, we’ll create new decoder states and outputs:\n",
    "\n",
    "```\n",
    "decoder_outputs, state_hidden, state_cell = \n",
    "    decoder_lstm(decoder_inputs, \n",
    "    initial_state=decoder_states_inputs)\n",
    " \n",
    "# Saving the new LSTM output states:\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "```\n",
    "\n",
    "4. Redefine the decoder output by passing it through the dense layer:\n",
    "\n",
    "```decoder_outputs = decoder_dense(decoder_outputs)```\n",
    "\n",
    "5. Set up the decoder model. This is where we bring together:\n",
    "\n",
    "* the decoder inputs (the decoder input layer)\n",
    "* the decoder input states (the final states from the encoder)\n",
    "* the decoder outputs (the NumPy matrix we get from the final output layer of the decoder)\n",
    "* the decoder output states (the memory throughout the network from one word to the next)\n",
    "\n",
    "```decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ef45da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder test model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "# Building the two decoder state input layers\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "\n",
    "# Put the state input layers into a list\n",
    "decoder_states_inputs = [decoder_state_input_hidden,\n",
    "  decoder_state_input_cell]\n",
    "\n",
    "# Call the decoder LSTM\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Redefine the decoder outputs\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Build the decoder test model\n",
    "decoder_model = Model(\n",
    "  [decoder_inputs] + decoder_states_inputs,\n",
    "  [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c9f23",
   "metadata": {},
   "source": [
    "<a name=\"TestFunction\"> </a>\n",
    "### The Test Function (part 1)\n",
    "1. Build a function that:\n",
    "    1. Accepts a NumPy matrix representing the test English sentence input\n",
    "    2. Uses the encoder and decoder we’ve created to generate Spanish output\n",
    "\n",
    "Inside the test function, we’ll run our new English sentence through the encoder model. The ```.predict()``` method takes in new input (as a NumPy matrix) and gives us output states that we can pass on to the decoder:\n",
    "\n",
    "```states = encoder.predict(test_input)```  \n",
    "*(test_input is a NumPy matrix representing an English sentence)*\n",
    "\n",
    "2. Build an empty NumPy array for our Spanish translation, giving it three dimensions:\n",
    "\n",
    "```target_sequence = np.zeros((1, 1, num_decoder_tokens))```  \n",
    "*(batch size: 1, number of tokens to start with: 1, number of tokens in our target vocabulary)*\n",
    "\n",
    "We already know the first value in our Spanish sentence — ```\"<Start>\"```, so we can give ```\"<Start>\"``` a value of 1 at the first timestep:\n",
    "\n",
    "```target_sequence[0, 0, target_features_dict['<START>']] = 1.```\n",
    "    \n",
    "Before we get decoding, we’ll need a string where we can add our translation to, word by word:\n",
    "\n",
    "```decoded_sentence = ''```\n",
    "    \n",
    "This is the variable that we will ultimately return from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "136cfa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: We'll see.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: We'll see.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: We'll try.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: We've won!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Well done.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: What's up?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Who cares?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Who drove?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Who drove?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Who is he?\n",
      "Decoded sentence: \n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(test_input):\n",
    "    # Encode the input as state vectors:\n",
    "    encoder_states_value = encoder_model.predict(test_input)\n",
    "    \n",
    "    # Set decoder states equal to encoder final states\n",
    "    decoder_states_value = encoder_states_value\n",
    "    \n",
    "    # Generate empty target sequence of length 1:\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "\n",
    "    # Populate the first token of target sequence with the start token:\n",
    "    target_seq[0, 0 , target_features_dict['<START>']] = 1.\n",
    "\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(10):\n",
    "    test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(test_input)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_docs[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55914153",
   "metadata": {},
   "source": [
    "### Test Function (part 2)\n",
    "**Translation time**:\n",
    "1. Decode the sentence word by word using the output state that we retrieved from the encoder (which becomes our decoder’s initial hidden state). \n",
    "2. Update the decoder hidden state after each word so that we use previously decoded words to help decode new ones.\n",
    "\n",
    "To tackle one word at a time, we need a while loop that will run until one of two things happens (we don’t want the model generating words forever):\n",
    "\n",
    "    1. The current token is ```\"<END>\"```.\n",
    "    2. The decoded Spanish sentence length hits the maximum target sentence length.  \n",
    "    \n",
    "Inside the while loop, the decoder model can use the current target sequence (beginning with the ```\"<START>\"``` token) and the current state (initially passed to us from the encoder model) to get a bunch of possible next words and their corresponding probabilities. In Keras, it looks something like this:\n",
    "\n",
    "```output_tokens, new_decoder_hidden_state, new_decoder_cell_state = decoder_model.predict([target_seq] + decoder_states_value)```  \n",
    "    \n",
    "3. Use NumPy’s .argmax() method to determine the token (word) with the highest probability and add it to the decoded sentence:\n",
    "\n",
    "```sampled_token_index = np.argmax(output_tokens[0, -1, :])```  \n",
    "*(slicing [0, -1, :] gives us a specific token vector within the 3d NumPy matrix)*   \n",
    "```sampled_token = reverse_target_features_dict[sampled_token_index]```  \n",
    "*(the reverse features dictionary translates back from index to Spanish)*  \n",
    "```decoded_sentence += \" \" + sampled_token```  \n",
    "\n",
    "4. Update a few values for the next word in the sequence:\n",
    "\n",
    "```target_seq = np.zeros((1, 1, num_decoder_tokens))```  \n",
    "```target_seq[0, 0, sampled_token_index] = 1.```  \n",
    "*(move to the next timestep of the target sequence)*  \n",
    "```decoder_states_value = [new_decoder_hidden_state, new_decoder_cell_state]```  \n",
    "*(update the states with values from the most recent decoder prediction)*  \n",
    "\n",
    "And now we can test it all out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3707ab91",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1472/2797784766.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mtest_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input sentence:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1472/2797784766.py\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(test_input)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# Run the decoder model to get possible output tokens (with probabilities) & states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0moutput_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_decoder_hidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_decoder_cell_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdecoder_states_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Choose token with highest probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1756\u001b[0m               stacklevel=2)\n\u001b[0;32m   1757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1758\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1759\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1760\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1401\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1153\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1154\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     dataset = dataset.map(\n\u001b[0m\u001b[0;32m    360\u001b[0m         grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2004\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2005\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2006\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   2007\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5499\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5500\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5501\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   5502\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4533\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4534\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3242\u001b[0m          \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3243\u001b[0m     \"\"\"\n\u001b[1;32m-> 3244\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   3245\u001b[0m         *args, **kwargs)\n\u001b[0;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3210\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3389\u001b[0m     ]\n\u001b[0;32m   3390\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3391\u001b[1;33m     graph_function = ConcreteFunction(\n\u001b[0m\u001b[0;32m   3392\u001b[0m         func_graph_module.func_graph_from_py_func(\n\u001b[0;32m   3393\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     \u001b[1;31m# These each get a reference to the FuncGraph deleter since they use the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[1;31m# FuncGraph directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1593\u001b[1;33m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0m\u001b[0;32m   1594\u001b[0m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0;32m   1595\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[0;32m    681\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m     self._inference_function = _EagerDefinedFunction(\n\u001b[0m\u001b[0;32m    684\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m       \u001b[0moutput_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m     fn = pywrap_tf_session.TF_GraphToFunction_wrapper(\n\u001b[0m\u001b[0;32m    477\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def decode_sequence(test_input):\n",
    "    encoder_states_value = encoder_model.predict(test_input)\n",
    "    decoder_states_value = encoder_states_value\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        # Run the decoder model to get possible output tokens (with probabilities) & states\n",
    "        output_tokens, new_decoder_hidden_state, new_decoder_cell_state = decoder_model.predict([target_seq] + decoder_states_value)\n",
    "\n",
    "        # Choose token with highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        # Exit condition: either hit max length or find stop token.\n",
    "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "        stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        # Update states\n",
    "        decoder_states_value = [new_decoder_hidden_state,\n",
    "        new_decoder_cell_state]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(10):\n",
    "    test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(test_input)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_docs[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663c4c2",
   "metadata": {},
   "source": [
    "The program can be improved by:\n",
    "1. using a larger data set\n",
    "2. increasing the size of the model\n",
    "3. adding more epochs for training\n",
    "4. convert the one-hot vectors into word embeddings during training. \n",
    "\n",
    "Using embeddings of words rather than one-hot vectors would help the model capture that semantically similar words might have semantically similar embeddings (helping the LSTM generalize)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
