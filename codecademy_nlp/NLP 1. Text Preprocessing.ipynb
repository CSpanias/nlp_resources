{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a9d9b4",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "1. [Noise Removal](#NoiseRemoval)\n",
    "2. [Tokenization](#Tokenization)\n",
    "3. [Normalization](#Normalization)\n",
    "    * Lower-case Letters\n",
    "    * Stopword Removal\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "    * Part-of-Speed (POS) Tagging\n",
    "    * Lemmatization with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf48f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text \n",
    "nlp_wiki = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and \n",
    "artificial intelligence concerned with the interactions between computers and human language, \n",
    "in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual \n",
    "nuances of the language within them. The technology can then accurately extract information \n",
    "and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "I am, as well as we are, the best!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94a69a",
   "metadata": {},
   "source": [
    "<a name=\"NoiseRemoval\"> </a>\n",
    "## Noise removal\n",
    "```sub()``` from ```re``` library  \n",
    "\n",
    "It takes three arguments:\n",
    "1. **Pattern** *a regex. There must be an ```r``` precding the string to indicate it is a raw string which treats ```\\``` as literals*\n",
    "2. **Replacement Text** *replaces all the matches in the input string*\n",
    "3. **Input** *the string that will be edited*\n",
    "\n",
    "Tasks:\n",
    "1. Remove **punctuation**\n",
    "2. Remove **extra whitespace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a005e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Natural language processing NLP is a subfield of linguistics computer science and \n",
      "artificial intelligence concerned with the interactions between computers and human language \n",
      "in particular how to program computers to process and analyze large amounts of natural language data \n",
      "The goal is a computer capable of understanding the contents of documents including the contextual \n",
      "nuances of the language within them The technology can then accurately extract information \n",
      "and insights contained in the documents as well as categorize and organize the documents themselves\n",
      "I am as well as we are the best\n",
      "\n",
      "\n",
      "\n",
      "Natural language processing NLP is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data The goal is a computer capable of understanding the contents of documents including the contextual nuances of the language within them The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves\n",
      "I am as well as we are the best\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# noise removal\n",
    "import re\n",
    "\n",
    "# remove puntucation\n",
    "nlp_no_punc = re.sub(r'[\\.\\,\\\"\\(\\)\\!]', '', nlp_wiki)\n",
    "\n",
    "# removing whitespace\n",
    "nlp_no_punc_whitespace = re.sub(r'\\s{2}', ' ', nlp_no_punc)\n",
    "\n",
    "# print text without punctuation\n",
    "print(nlp_no_punc)\n",
    "print(\"\")\n",
    "# print text without punc and whitespace\n",
    "print(nlp_no_punc_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028a80c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\10inm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk to Jupyter Notebook\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dadfa11",
   "metadata": {},
   "source": [
    "<a name=\"Tokenization\"> </a>\n",
    "## Tokenization\n",
    "```word_tokenize()``` from ```nltk.tokenize```\n",
    "\n",
    "Convert the string into a **list of tokens** (individual words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6303bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', 'understanding', 'the', 'contents', 'of', 'documents', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', 'I', 'am', 'as', 'well', 'as', 'we', 'are', 'the', 'best']\n"
     ]
    }
   ],
   "source": [
    "#nlkt.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# split the text into individual words\n",
    "nlp_wiki_tokenized = word_tokenize(nlp_no_punc_whitespace)\n",
    "\n",
    "# print the list of words\n",
    "print(nlp_wiki_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff35853",
   "metadata": {},
   "source": [
    "<a name=\"Normalization\"> </a>\n",
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfb1fb",
   "metadata": {},
   "source": [
    "### Lower-case words\n",
    "```lower()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e59083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', 'the', 'goal', 'is', 'a', 'computer', 'capable', 'of', 'understanding', 'the', 'contents', 'of', 'documents', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', 'the', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', 'i', 'am', 'as', 'well', 'as', 'we', 'are', 'the', 'best']\n"
     ]
    }
   ],
   "source": [
    "# for word in the list of words lowercase the word\n",
    "nlp_wiki_lower = [word.lower() for word in nlp_wiki_tokenized]\n",
    "\n",
    "# print the new list\n",
    "print(nlp_wiki_lower) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97253bb",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "```stopwords``` from ```nltk.corpus```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3d94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10inm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1963d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', 'goal', 'computer', 'capable', 'understanding', 'contents', 'documents', 'including', 'contextual', 'nuances', 'language', 'within', 'technology', 'accurately', 'extract', 'information', 'insights', 'contained', 'documents', 'well', 'categorize', 'organize', 'documents', 'well', 'best']\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# define set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove stopwords from text\n",
    "nlp_wiki_no_stop = [word for word in nlp_wiki_lower\n",
    "                    if word not in stop_words]\n",
    "\n",
    "# print list of words\n",
    "print(nlp_wiki_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de57f78",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "```PorterStemmer``` from ```nltk.stem```\n",
    "\n",
    "*Removing word affixes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4455a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', 'goal', 'comput', 'capabl', 'understand', 'content', 'document', 'includ', 'contextu', 'nuanc', 'languag', 'within', 'technolog', 'accur', 'extract', 'inform', 'insight', 'contain', 'document', 'well', 'categor', 'organ', 'document', 'well', 'best']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# instantiate PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stem the words\n",
    "nlp_wiki_stemmed = [stemmer.stem(token) for token in nlp_wiki_no_stop]\n",
    "\n",
    "# print the new list\n",
    "print(nlp_wiki_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d3cf0",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "```WordNetLemmatizer``` from ```nltk.stem```\n",
    "\n",
    "*Casting word to their root form*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7481a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\10inm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee4517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', 'goal', 'computer', 'capable', 'understanding', 'content', 'document', 'including', 'contextual', 'nuance', 'language', 'within', 'technology', 'accurately', 'extract', 'information', 'insight', 'contained', 'document', 'well', 'categorize', 'organize', 'document', 'well', 'best']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# instantiate the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize the words\n",
    "nlp_wiki_lemmatized = [lemmatizer.lemmatize(token) \n",
    "                       for token in nlp_wiki_no_stop]\n",
    "\n",
    "# print the new list\n",
    "print(nlp_wiki_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9274d",
   "metadata": {},
   "source": [
    "```lemmatize``` treats every word as a **noun**!\n",
    "\n",
    "To take advantage of lemmatization we need to **perform POS-tagging first!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b37b8",
   "metadata": {},
   "source": [
    "### Part-of-Speed (POS) Tagging\n",
    "```wordnet``` from ```nltk.corpus``` *a database for contextualizing words*  \n",
    "```Counter``` from ```collections``` *a container that stores elements as dictionary keys*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b53750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def pos_tagging(token):\n",
    "    \"\"\"It identifies the most probable part of speech of a word.\"\"\"\n",
    "    \n",
    "    # get a set of POS-tagged synonyms for the word\n",
    "    probable_pos = wordnet.synsets(token)\n",
    "    \n",
    "    # instantiate Counter\n",
    "    pos_counts = Counter()\n",
    "    \n",
    "    # count the number of nouns in the syn set\n",
    "    pos_counts['n'] = len([item for item in probable_pos if item.pos() == 'n'])\n",
    "    # count the number of verbs in the syn set\n",
    "    pos_counts['v'] = len([item for item in probable_pos if item.pos() == 'v'])\n",
    "    # count the number of adjectives in the syn set\n",
    "    pos_counts['a'] = len([item for item in probable_pos if item.pos() == 'a'])\n",
    "    # count the number of adverbs in the syn set\n",
    "    pos_counts['r'] = len([item for item in probable_pos if item.pos() == 'r'])\n",
    "    \n",
    "    most_likely_pos = pos_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return most_likely_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf5e99a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', 'goal', 'computer', 'capable', 'understanding', 'content', 'document', 'including', 'contextual', 'nuance', 'language', 'within', 'technology', 'accurately', 'extract', 'information', 'insight', 'contained', 'document', 'well', 'categorize', 'organize', 'document', 'well', 'best']\n",
      "\n",
      "['natural', 'language', 'process', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concern', 'interaction', 'computer', 'human', 'language', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', 'goal', 'computer', 'capable', 'understand', 'content', 'document', 'include', 'contextual', 'nuance', 'language', 'within', 'technology', 'accurately', 'extract', 'information', 'insight', 'contain', 'document', 'well', 'categorize', 'organize', 'document', 'well', 'best']\n"
     ]
    }
   ],
   "source": [
    "# perform lemmatization along with POS tagging\n",
    "nlp_wiki_pos_lemmatized = [lemmatizer.lemmatize(token, pos_tagging(token)) for token in nlp_wiki_no_stop]\n",
    "\n",
    "# compare lemmatization without and with POS tagging\n",
    "print(nlp_wiki_lemmatized)\n",
    "print(\"\")\n",
    "print(nlp_wiki_pos_lemmatized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
