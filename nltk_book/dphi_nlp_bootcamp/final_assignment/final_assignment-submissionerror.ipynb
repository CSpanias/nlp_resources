{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915c7924",
   "metadata": {},
   "source": [
    "# [Getting Started with NLP](https://dphi.tech/bootcamps/getting-started-with-natural-language-processing?utm_source=header)\n",
    "by [CSpanias](https://cspanias.github.io/aboutme/), 28/01 - 06/02/2022 <br>\n",
    "\n",
    "Bootcamp organized by **[DPhi](https://dphi.tech/community/)**, lectures given by [**Dipanjan (DJ) Sarkar**](https://www.linkedin.com/in/dipanzan/) ([GitHub repo](https://github.com/dipanjanS/nlp_essentials)) <br>\n",
    "\n",
    "This notebook constitutes my **personal submission** to the final assignment of the Bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ec43b",
   "metadata": {},
   "source": [
    "# CONTENT\n",
    "1. [Problem Overview](#ProblemOverview)\n",
    "2. [Import & Check Dataset](#Data)\n",
    "    1. [Missing Values](#nans)\n",
    "    1. [Duplicated Rows](#duplicates)\n",
    "    1. [Balance](#balance)\n",
    "3. [NLP Pipeline](#Pipeline)\n",
    "    1. [Text Pre-Processing](#TextPre)\n",
    "    1. [Splitting Dataset](#SplitData)\n",
    "    1. [Basic NLP Count-Based Features](#NLPCB)\n",
    "    1. [Build a Classification Model](#MLModel)\n",
    "    1. [Hyperparameter Optimization](#GS)\n",
    "    1. [Logistic Regression](#LogReg)\n",
    "4. [Conclusion](#conclusion)\n",
    "5. [Submission](#submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbf561",
   "metadata": {},
   "source": [
    "<a name=\"ProblemOverview\"></a>\n",
    "# 1. Problem Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be953a5",
   "metadata": {},
   "source": [
    "> In this challenge, you will work on a dataset that contains **news headlines** - which are aimed to be **written in a sarcastic manner** by the news author. Our job here is to build our NLP models and **predict whether the headline is sarcastic or not**.\n",
    "\n",
    "This problem represents a **binary classification problem** as the news headlines need to be **classified betweeen 2 categories**:\n",
    "1. Sarcastic (1)\n",
    "2. Not Sarcastic (0)\n",
    "\n",
    "_More info about different Classification types [here](https://machinelearningmastery.com/types-of-classification-in-machine-learning/#:~:text=In%20machine%20learning%2C%20classification%20refers,one%20of%20the%20known%20characters.)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb581479",
   "metadata": {},
   "source": [
    "<a name=\"Data\"></a>\n",
    "# 2. Import & Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa119f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd # import dataset, create and manipulate dataframes\n",
    "import numpy as np # vectorize functions and perform calculations\n",
    "import contractions # expand contractions\n",
    "import re # regular expressions\n",
    "import string # count-based features\n",
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "\n",
    "from nltk.tokenize import word_tokenize # tokenize string or sentences\n",
    "from nltk.corpus import stopwords # import english stopword list\n",
    "from nltk.stem import PorterStemmer # stemming\n",
    "from sklearn.linear_model import LogisticRegression # our algorithm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # count-based language models\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer # model evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score # model evaluation metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # split & evaluate dataset, hyperparameter optimization\n",
    "from sklearn.model_selection import KFold # cross-validation\n",
    "from collections import Counter # count-based calculations\n",
    "from textblob import TextBlob # sentiment analysis\n",
    "from wordcloud import WordCloud # visualization\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # hide warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c755c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset as dataframe\n",
    "df_train = pd.read_csv('https://github.com/CSpanias/nlp_resources/blob/main/dphi_nlp_bootcamp/final_assigment/Train_Dataset.csv?raw=true')\n",
    "df_test = pd.read_csv('https://github.com/CSpanias/nlp_resources/blob/main/dphi_nlp_bootcamp/final_assigment/Test_Dataset.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5466cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated rows\n",
    "df_train.drop_duplicates(keep='first', inplace=True)\n",
    "df_test.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f10512",
   "metadata": {},
   "source": [
    "<a name=\"Pipeline\"></a>\n",
    "# 3. NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a13b53",
   "metadata": {},
   "source": [
    "The steps below will form our **NLP pipeline** for building our NLP models:\n",
    "1. [Text Pre-Processing](#TextPre)\n",
    "1. [Train & Test Datasets](#SplitData)\n",
    "1. [Basic NLP Count-Based Features](#NLPCB)\n",
    "1. [Sentiment Analysis](#sentana)\n",
    "1. [Bag of Words](#BoW)\n",
    "1. [Build a Classification Model](#MLModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd90fd",
   "metadata": {},
   "source": [
    "<a name=\"TextPre\"></a>\n",
    "## 3.1 Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b0973",
   "metadata": {},
   "source": [
    "Normally, our $1^{st}$ step would be to perform some **basic text pre-processing** like:\n",
    "* remove stopwords\n",
    "* remove punctuation\n",
    "* lower case characters\n",
    "* stip whitespace\n",
    "* expand contractions\n",
    "\n",
    "In this case **stopwords**, **punctuation** as well as **character casing** could provide information regarding the **tone of the headline**, thus we will keep them as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033ee9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords default nltk list\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    \"\"\"Normalize the document by performing basic text pre-processing tasks.\"\"\"\n",
    "\n",
    "    # remove special characters\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    # remove trailing whitespace\n",
    "    nowhite = doc.strip()\n",
    "    # expand contractions\n",
    "    expanded = contractions.fix(nowhite)\n",
    "    # tokenize document\n",
    "    tokens = word_tokenize(expanded)\n",
    "    # remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# vectorize function for faster computations\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f022ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize 'headline'\n",
    "norm_corpus_train = normalize_corpus(list(df_train['headline']))\n",
    "norm_corpus_test = normalize_corpus(list(df_test['headline']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682f163",
   "metadata": {},
   "source": [
    "<a name=\"SplitData\"></a>\n",
    "## 3.2 Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7999143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign feature & target variables\n",
    "X = df_train.drop(['is_sarcastic'], axis = 1)\n",
    "y = df_train['is_sarcastic']\n",
    "\n",
    "# split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2a6b0",
   "metadata": {},
   "source": [
    "<a name=\"NLPCB\"></a>\n",
    "## 3.3 Basic NLP Count-based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54c80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total number of characters\n",
    "X_train['char_count'] = X_train['headline'].apply(len)\n",
    "# calculate total number of words\n",
    "X_train['word_count'] = X_train['headline'].apply(lambda x: len(x.split()))\n",
    "# # calculate average word density\n",
    "X_train['word_density'] = X_train['char_count'] / (X_train['word_count']+1)\n",
    "# calculate total number of punctuaction marks\n",
    "X_train['punctuation_count'] = X_train['headline'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "# calculate total number of title-cased words\n",
    "X_train['title_word_count'] = X_train['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "# calculate total number of upper-cased words\n",
    "X_train['upper_case_word_count'] = X_train['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "# calculate total number of characters\n",
    "X_test['char_count'] = X_test['headline'].apply(len)\n",
    "# calculate total number of words\n",
    "X_test['word_count'] = X_test['headline'].apply(lambda x: len(x.split()))\n",
    "# calculate average word density\n",
    "X_test['word_density'] = X_test['char_count'] / (X_test['word_count']+1)\n",
    "# calculate total number of punctuaction marks\n",
    "X_test['punctuation_count'] = X_test['headline'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "# calculate total number of title-cased words\n",
    "X_test['title_word_count'] = X_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "# calculate total number of upper-cased words\n",
    "X_test['upper_case_word_count'] = X_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "# calculate total number of characters\n",
    "df_test['char_count'] = df_test['headline'].apply(len)\n",
    "# calculate total number of words\n",
    "df_test['word_count'] = df_test['headline'].apply(lambda x: len(x.split()))\n",
    "# calculate average word density\n",
    "df_test['word_density'] = df_test['char_count'] / (X_test['word_count']+1)\n",
    "# calculate total number of punctuaction marks\n",
    "df_test['punctuation_count'] = df_test['headline'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "# calculate total number of title-cased words\n",
    "df_test['title_word_count'] = df_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "# calculate total number of upper-cased words\n",
    "df_test['upper_case_word_count'] = df_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6cb6d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove columns\n",
    "X_train.drop(columns=['title_word_count', 'upper_case_word_count'], inplace=True, axis=0)\n",
    "X_test.drop(columns=['title_word_count', 'upper_case_word_count'], inplace=True, axis=0)\n",
    "df_test.drop(columns=['title_word_count', 'upper_case_word_count'], inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24272973",
   "metadata": {},
   "source": [
    "<a name=\"sentana\"></a>\n",
    "## 3.4 Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e862889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate review's sentiment \n",
    "x_train_snt_obj = X_train['headline'].apply(lambda row: TextBlob(row).sentiment)\n",
    "# create a column for polarity scores\n",
    "X_train['Polarity'] = [obj.polarity for obj in x_train_snt_obj.values]\n",
    "# create a column for subjectivity scores\n",
    "X_train['Subjectivity'] = [obj.subjectivity for obj in x_train_snt_obj.values]\n",
    "\n",
    "# calculate review's sentiment \n",
    "x_test_snt_obj = X_test['headline'].apply(lambda row: TextBlob(row).sentiment)\n",
    "# create a column for polarity scores\n",
    "X_test['Polarity'] = [obj.polarity for obj in x_test_snt_obj.values]\n",
    "# create a column for subjectivity scores\n",
    "X_test['Subjectivity'] = [obj.subjectivity for obj in x_test_snt_obj.values]\n",
    "\n",
    "# calculate review's sentiment \n",
    "df_test_snt_obj = df_test['headline'].apply(lambda row: TextBlob(row).sentiment)\n",
    "# create a column for polarity scores\n",
    "df_test['Polarity'] = [obj.polarity for obj in df_test_snt_obj.values]\n",
    "# create a column for subjectivity scores\n",
    "df_test['Subjectivity'] = [obj.subjectivity for obj in df_test_snt_obj.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e2946",
   "metadata": {},
   "source": [
    "<a name=\"BoW\"></a>\n",
    "## 3.5 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d64572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords default nltk list\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# load up a simple porter stemmer - nothing fancy\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def simple_text_preprocessor(document):\n",
    "    \"\"\"Perform basic text pre-processing tasks.\"\"\"\n",
    "    \n",
    "    # lower case\n",
    "    document = str(document).lower()\n",
    "    \n",
    "    # expand contractions\n",
    "    document = contractions.fix(document)\n",
    "    \n",
    "    # remove unnecessary characters\n",
    "    document = re.sub(r'[^a-zA-Z]',r' ', document)\n",
    "    document = re.sub(r'nbsp', r'', document)\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    \n",
    "    # simple porter stemming\n",
    "    document = ' '.join([ps.stem(word) for word in document.split()])\n",
    "    \n",
    "    # stopwords removal\n",
    "    document = ' '.join([word for word in document.split() if word not in stop_words])\n",
    "    \n",
    "    return document\n",
    "\n",
    "# vectorize function\n",
    "stp = np.vectorize(simple_text_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454d55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with cleaned text\n",
    "X_train['Clean Headline'] = stp(X_train['headline'].values)\n",
    "X_test['Clean Headline'] = stp(X_test['headline'].values)\n",
    "df_test['Clean Headline'] = stp(df_test['headline'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dffc7891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove the 2 columns\n",
    "X_train_metadata = X_train.drop(['headline', 'Clean Headline'], axis=1).reset_index(drop=True)\n",
    "X_test_metadata = X_test.drop(['headline', 'Clean Headline'], axis=1).reset_index(drop=True)\n",
    "df_test_metadata = df_test.drop(['headline', 'Clean Headline'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382e3948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # instatiate vectorizer\n",
    "# cv = CountVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 1))\n",
    "\n",
    "# # fit vectorizer to 'Clean Review' and convert it to numpy array\n",
    "# X_traincv = cv.fit_transform(X_train['Clean Headline']).toarray()\n",
    "# # create a pandas DataFrame\n",
    "# X_traincv = pd.DataFrame(X_traincv, columns=cv.get_feature_names())\n",
    "\n",
    "# # use vectorizer to transform 'Clean Review' and convert it to numpy array\n",
    "# X_testcv = cv.transform(X_test['Clean Headline']).toarray()\n",
    "# # create a pandas DataFrame\n",
    "# X_testcv = pd.DataFrame(X_testcv, columns=cv.get_feature_names())\n",
    "\n",
    "# # check first 5 rows\n",
    "# X_traincv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88aafbf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # instatiate vectorizer\n",
    "# cv = CountVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 1))\n",
    "\n",
    "# # fit vectorizer to 'Clean Review' and convert it to numpy array\n",
    "# X_traincv = cv.fit_transform(X_train['Clean Headline']).toarray()\n",
    "# # create a pandas DataFrame\n",
    "# X_traincv = pd.DataFrame(X_traincv, columns=cv.get_feature_names())\n",
    "\n",
    "# # use vectorizer to transform 'Clean Review' and convert it to numpy array\n",
    "# df_testcv = cv.transform(df_test['Clean Headline']).toarray()\n",
    "# # create a pandas DataFrame\n",
    "# df_testcv = pd.DataFrame(df_testcv, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b8e14",
   "metadata": {},
   "source": [
    "We now must **concatenate the 2 seperate DataFrames into a single DataFrame**, and **remove `headline` column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a138834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # concatenate the 2 dataframes\n",
    "# X_train_comb = pd.concat([X_train_metadata, X_traincv], axis=1)\n",
    "# df_test_comb = pd.concat([df_test_metadata, df_testcv], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4498679",
   "metadata": {},
   "source": [
    "<a name=\"LogReg\"></a>\n",
    "## 3.7 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5061165",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# instantiate log reg\n",
    "lr = LogisticRegression(C=1, random_state=42, solver='liblinear')\n",
    "# train logreg\n",
    "lr.fit(X_train_metadata, y_train)\n",
    "# predict using test data\n",
    "target = lr.predict(X_test_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38248902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.67      4711\n",
      "           1       0.61      0.49      0.54      4142\n",
      "\n",
      "    accuracy                           0.61      8853\n",
      "   macro avg       0.61      0.61      0.60      8853\n",
      "weighted avg       0.61      0.61      0.61      8853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "print(classification_report(y_test, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b5adc",
   "metadata": {},
   "source": [
    "<a name=\"submission\"></a>\n",
    "# 5. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68040d",
   "metadata": {},
   "source": [
    "You can read [this](https://discuss.dphi.tech/t/how-to-submit-predictions-in-datathons-data-sprints-on-dphi/548) post which includes **details regarding the submission process**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bec00f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 8853 elements, new values have 11066 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15028/2585111256.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# its important for comparison\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5498\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5499\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5500\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5501\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5502\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     58\u001b[0m                 \u001b[1;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;34mf\"values have {new_len} elements\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 8853 elements, new values have 11066 elements"
     ]
    }
   ],
   "source": [
    "# predictions is nothing but the final predictions of your model on input features of your new unseen test data\n",
    "res = pd.DataFrame(target) \n",
    "\n",
    "# its important for comparison\n",
    "res.index = df_test.index \n",
    "res.columns = [\"prediction\"]\n",
    "\n",
    "# the csv file will be saved locally on the same location where this notebook is located\n",
    "res.to_csv(\"prediction_results.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
