{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Getting Started with NLP](https://dphi.tech/bootcamps/getting-started-with-natural-language-processing?utm_source=header)\n",
    "by [CSpanias](https://cspanias.github.io/aboutme/), 28/01 - 06/02/2022 <br>\n",
    "\n",
    "Bootcamp organized by **[DPhi](https://dphi.tech/community/)**, lectures given by [**Dipanjan (DJ) Sarkar**](https://www.linkedin.com/in/dipanzan/) ([GitHub repo](https://github.com/dipanjanS/nlp_essentials)) <br>\n",
    "\n",
    "## Fundamental Tutorials for NLP:\n",
    "* [NLTK Book](https://www.nltk.org/book/)\n",
    "* [spaCy Tutorials](https://course.spacy.io/en/chapter1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT\n",
    "\n",
    "1. Text Wrangling\n",
    "    1. [Step-by-Step](#Steps)\n",
    "        1. [Tokenization](#Tokenization)\n",
    "        2. [Removing HTML tags & Noise](#Noise)\n",
    "        3. [Removing Accented Characters](#Accented)\n",
    "        4. [Removing Special Characters, Numbers and Symbols](#Special)\n",
    "        5. [Expanding Contractions](#Contractions)\n",
    "        6. [Stemming](#Stemming)\n",
    "        7. [Lemmatization](#Lemmatization)\n",
    "        8. [Stopword Removal](#Stopwords)\n",
    "    1. [Automate Lemmatization](#AutoLemm)\n",
    "        1. [Lemmatization with NLTK](#NLTKProcess)\n",
    "        2. [Lemmatization with spaCy](#SpacyProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "ceSG71XiJoka",
    "outputId": "c1fa4f8d-9143-42bc-ef76-07166fc0710c"
   },
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#!pip install contractions\n",
    "#!pip install textsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Steps\"></a>\n",
    "# 1.1 Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3GzHq46JFW_"
   },
   "source": [
    "<a name=\"Tokenization\"></a>\n",
    "## 1.1.1 Tokenization\n",
    " The process of **splitting a string** into a list of **sentences** or **words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zIiPr5JBJFXA",
    "outputId": "02958ece-7022-4a28-9591-d8caef757b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our sample text of type <class 'str'>:\n",
      "\n",
      "US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\n"
     ]
    }
   ],
   "source": [
    "# create text as a single multiline string\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "\n",
    "# print sample text\n",
    "print(\"This is our sample text of type {}:\\n\\n{}\".format(type(sample_text), sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with NLTK\n",
    "Split a string into a list of **sentences**.\n",
    "\n",
    "**`from nltk.tokenize import sent_tokenize`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "i2m8nEPmJFXD",
    "outputId": "83fe372f-8901-43d1-d6f5-fed5ee27cd91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US unveils world's most powerful supercomputer, beats China.\n"
     ]
    }
   ],
   "source": [
    "# split a string into sentences and print first sentence\n",
    "print(nltk.sent_tokenize(sample_text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split a string into a list of **words**.\n",
    "\n",
    "**`from nltk.tokenize import word_tokenize`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "KjVNIwLoJFXG",
    "outputId": "093b33f0-6449-4fd0-c1ed-1dfc9bb6e318",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China']\n"
     ]
    }
   ],
   "source": [
    "# split a string into tokens and print the first 10 tokens\n",
    "print(nltk.word_tokenize(sample_text)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a **spaCy pipeline**.\n",
    "\n",
    "**`spacy.load('en_core_web_sm')`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjhORAuPJFXL"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load a pipeline using the name of an installed package\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an **NLP object**.\n",
    "\n",
    "**`nlp(text)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tokens are: \"US unveils world's most\".\n",
      "\n",
      "The type of the NLP construct is: <class 'spacy.tokens.doc.Doc'>.\n",
      "\n",
      "The length of the NLP construct is: 84.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create an NLP construct\n",
    "text_spacy = nlp(sample_text)\n",
    "\n",
    "# print NLP construct\n",
    "print(\"The first 5 tokens are: \\\"{}\\\".\\n\".format(text_spacy[:5]))\n",
    "\n",
    "# check type\n",
    "print(\"The type of the NLP construct is: {}.\\n\".format(type(text_spacy)))\n",
    "\n",
    "# check length\n",
    "print(\"The length of the NLP construct is: {}.\\n\".format(len(text_spacy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a string into **sentences**.\n",
    "\n",
    "**`obj.text for obj in text_object.sents`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "DR6LA_YHJFXN",
    "outputId": "630d9f19-8658-4b3b-bbd8-069fb3594601",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"US unveils world's most powerful supercomputer, beats China.\", \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"]\n"
     ]
    }
   ],
   "source": [
    "# tokenize text into sentences\n",
    "print([obj.text for obj in text_spacy.sents][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a string into **words**.\n",
    "\n",
    "**`obj.text for obj in text_object`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "DBuAHdR8JFXQ",
    "outputId": "f6acbbf6-e500-42c4-f06d-f1f8942ec548",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled']\n"
     ]
    }
   ],
   "source": [
    "# tokenize text into words\n",
    "print([obj.text for obj in text_spacy][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhxnJkIsJFXS"
   },
   "source": [
    "<a name=\"Noise\"></a>\n",
    "## 1.1.2 Removing HTML tags & noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "E3qV1WOpJFXT",
    "outputId": "82fbdc9e-df22-410d-bd33-1096b34647e6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* */\r\n",
      "hr {\r\n",
      "    width: 45%;\r\n",
      "    /* adjust to ape original work */\r\n",
      "    margin-top: 1em;\r\n",
      "    /* space above & below */\r\n",
      "    margin\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# request information from a site\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "\n",
    "# get the text of the contenct\n",
    "content = data.text\n",
    "\n",
    "# print a sample\n",
    "print(content[2745:2900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "E6UAz3mjJFXY",
    "outputId": "0549c5a6-1814-4e82-8046-d32b40647d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d of schedule]\n",
      "[This file was first posted on June 7, 2003]\n",
      "Edition: 10\n",
      "Language: English\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\n",
      "This eBook was produced by David Widger\n",
      "with the help of Derek Andrew's text from January 1992\n",
      "and the work of Bryan Taylor in November 2002.\n",
      "Book 01        Genesis\n",
      "01:001:001 In the beginning God created the heaven and the earth.\n",
      "01:001:002 And the earth was without form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "           light from the darkness.\n",
      "01:001:005 And God called the light Day, and the\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# function to remove HTML tags\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"Remove HTML tags and get just the text of a request.\"\"\"\n",
    "    \n",
    "    # instantiate BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    #\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    # get just the text without HTML tags\n",
    "    stripped_text = soup.get_text()\n",
    "    # \n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    #\n",
    "    return stripped_text\n",
    "\n",
    "# call function passing our request\n",
    "clean_content = strip_html_tags(content)\n",
    "# print sample text\n",
    "print(clean_content[1163:1957])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fJi5YyKJFXc"
   },
   "source": [
    "<a name=\"Accented\"></a>\n",
    "## 1.1.3 Removing Accented Characters\n",
    "\n",
    "**`unicodedata`** info [here](https://docs.python.org/3/library/unicodedata.html). <br>\n",
    "General info about to work with **Unicode** [here](https://docs.python.org/3/howto/unicode.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps9wmhv9JFXd"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"Remove accented characters from the text.\"\"\"\n",
    "    \n",
    "    # \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # return clean text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mc7JR8CQJFXh",
    "outputId": "392b46f9-4945-47e0-b942-9d50de2fb3ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Accented text\n"
     ]
    }
   ],
   "source": [
    "# create text with accented chars\n",
    "s = 'Sómě Áccěntěd těxt'\n",
    "# call function to clean text\n",
    "print(remove_accented_chars(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gj8CyGmPJFXr"
   },
   "source": [
    "<a name=\"Special\"></a>\n",
    "## 1.1.4 Removing Special Characters, Numbers and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dkc4ESDJFXs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well this was fun See you at  What do you think   \n",
      "\n",
      "Well this was fun See you at 730 What do you think 9318 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    \"\"\"Remove special characters from a text using regex.\"\"\"\n",
    "    \n",
    "    # create regex pattern\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    # remove text that matches pattern\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # return text without pattern\n",
    "    return text\n",
    "\n",
    "# create text with special characters\n",
    "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
    "# call function to clean text\n",
    "print(remove_special_characters(s, remove_digits=True), \"\\n\")\n",
    "# call function to clean text, but keep numbers\n",
    "print(remove_special_characters(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ho6h68QbJFYX"
   },
   "source": [
    "<a name=\"Contractions\"></a>\n",
    "## 1.1.5 Expanding Contractions\n",
    "**`import contractions`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5xWsgO-jJFYc",
    "outputId": "b02a9709-8e7e-4174-d46c-769d73aeadda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"I'm\", 'I am'), (\"I'm'a\", 'I am about to'), (\"I'm'o\", 'I am going to'), (\"I've\", 'I have'), (\"I'll\", 'I will'), (\"I'll've\", 'I will have'), (\"I'd\", 'I would'), (\"I'd've\", 'I would have'), ('Whatcha', 'What are you'), (\"amn't\", 'am not')] \n",
      "\n",
      "Original text:\n",
      "Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\n",
      "\n",
      "Expanded text:\n",
      "You all cannot expand contractions I would think! You would not be able to. How did you do it?\n"
     ]
    }
   ],
   "source": [
    "# create text with contractions\n",
    "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
    "\n",
    "import contractions\n",
    "\n",
    "# check the first 10 contraction pairs\n",
    "print(list(contractions.contractions_dict.items())[:10], \"\\n\")\n",
    "\n",
    "# expand contractions\n",
    "print(\"Original text:\\n{}\\n\".format(s))\n",
    "print(\"Expanded text:\\n{}\".format(contractions.fix(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeUHPmhDJFZC"
   },
   "source": [
    "<a name=\"Stemming\"></a>\n",
    "## 1.1.6 Stemming\n",
    "**`from nltk.stem import PorterStemmer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8ndJ4XOKJFZD",
    "outputId": "fb53058e-bbdf-489b-f57d-fbdb7e7395c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \"jumping\"\n",
      "Stemmed text: \"jump\"\n",
      "\n",
      "Original text: \"jumps\"\n",
      "Stemmed text: \"jump\"\n",
      "\n",
      "Original text: \"jumped\"\n",
      "Stemmed text: \"jump\"\n",
      "\n",
      "Original text: \"strange\"\n",
      "Stemmed text: \"strang\"\n",
      "\n",
      "Original text: \"lying\"\n",
      "Stemmed text: \"lie\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# instantiate Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# apply stemmer\n",
    "print('Original text: \"jumping\"\\nStemmed text: \"{}\"\\n'.format(ps.stem('jumping')))\n",
    "print('Original text: \"jumps\"\\nStemmed text: \"{}\"\\n'.format(ps.stem('jumps'))) \n",
    "print('Original text: \"jumped\"\\nStemmed text: \"{}\"\\n'.format(ps.stem('jumped')))\n",
    "print('Original text: \"strange\"\\nStemmed text: \"{}\"\\n'.format(ps.stem('strange')))\n",
    "print('Original text: \"lying\"\\nStemmed text: \"{}\"\\n'.format(ps.stem('lying')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQNUmpfLJFZu"
   },
   "source": [
    "<a name=\"Lemmatization\"></a>\n",
    "## 1.1.7 Lemmatization\n",
    "**`from nltk.stem import WordNetLemmatizer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16ygP7t1JFZv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize nouns ('n'):\n",
      "Original text: \"cars\"\n",
      "Lemmatized text: \"car\"\n",
      "\n",
      "Original text: \"boxes\"\n",
      "Lemmatized text: \"box\"\n",
      "\n",
      "Lemmatize verbs ('v'):\n",
      "Original text: \"running\"\n",
      "Lemmatized text: \"run\"\n",
      "\n",
      "Original text: \"ate\"\n",
      "Lemmatized text: \"eat\"\n",
      "\n",
      "Lemmatize adjectives ('a'):\n",
      "Original text: \"saddest\"\n",
      "Lemmatized text: \"sad\"\n",
      "\n",
      "Original text: \"fancier\"\n",
      "Lemmatized text: \"fancy\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# instantiate Lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize nouns\n",
    "print(\"Lemmatize nouns ('n'):\")\n",
    "print('Original text: \"cars\"\\nLemmatized text: \"{}\"\\n'.format(wnl.lemmatize('cars', 'n')))\n",
    "print('Original text: \"boxes\"\\nLemmatized text: \"{}\"\\n'.format(wnl.lemmatize('boxes', 'n')))\n",
    "\n",
    "# lemmatize verbs\n",
    "print(\"Lemmatize verbs ('v'):\")\n",
    "print('Original text: \"running\"\\nLemmatized text: \"{}\"\\n'.format(wnl.lemmatize('running', 'v')))\n",
    "print('Original text: \"ate\"\\nLemmatized text: \"{}\"\\n'.format(wnl.lemmatize('ate', 'v')))\n",
    "\n",
    "# lemmatize adjectives\n",
    "print(\"Lemmatize adjectives ('a'):\")\n",
    "print('Original text: \"saddest\"\\nLemmatized text: \"{}\"\\n'.format(wnl.lemmatize('saddest', 'a')))\n",
    "print('Original text: \"fancier\"\\nLemmatized text: \"{}\"\\n'.format(wnl.lemmatize('fancier', 'a')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQsKAXlvJFa7"
   },
   "source": [
    "<a name=\"Stopwords\"></a>\n",
    "## 1.1.8 Stopword Removal\n",
    "**`stop_words = nltk.corpus.stopwords.words('english')`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkJLKKxrJFa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words of the stopwords list are:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "\n",
      "Original text:\n",
      "The brown foxes are quick and they are jumping over the sleeping lazy dogs!\n",
      "\n",
      "Text with no stopwords:\n",
      "brown foxes quick jumping sleeping lazy dogs !\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(\"The first 10 words of the stopwords list are:\\n{}\\n\".format(stop_words[:10]))\n",
    "\n",
    "# call function\n",
    "print('Original text:\\n{}\\n'.format(s))\n",
    "print('Text with no stopwords:\\n{}'.format(remove_stopwords(s, is_lower_case=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bcnWSnAJFbG"
   },
   "source": [
    "We can **remove and/or add words** into the list as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPAM2rNZJFbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The brown foxes are quick and they are jumping over the sleeping lazy dogs!\n",
      "\n",
      "Text with no stopwords:\n",
      "The foxes quick jumping the sleeping lazy dogs !\n"
     ]
    }
   ],
   "source": [
    "# assign default stopword list to a variable\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# remove the word 'the'\n",
    "stop_words.remove('the')\n",
    "# add the word 'brown'\n",
    "stop_words.append('brown')\n",
    "\n",
    "# call function\n",
    "print('Original text:\\n{}\\n'.format(s))\n",
    "print('Text with no stopwords:\\n{}'.format(remove_stopwords(s, is_lower_case=False, stopwords=stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"AutoLemm\"></a>\n",
    "# 1.2 Automate Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQ1S2ngz7B84"
   },
   "source": [
    "<a name=\"NLTKProcess\"></a>\n",
    "## 1.2.1 Lemmatization with NLTK\n",
    "\n",
    "### Process\n",
    "`tokenization` &rarr; `POS-tagging` &rarr; `WordNet-tagging` &rarr; `lemmatization` <br>\n",
    "\n",
    "### Corresponding functions\n",
    "`word_tokenize(text)` &rarr; `pos_tag(tokens)` &rarr; `pos_tag_wordnet(tagged_tokens)` &rarr; `WordNetLemmatizer.lemmatize(tagged_tokens)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4g85bOGJFaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The brown foxes are quick and they are jumping over the sleeping lazy dogs!\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5260/2944381155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# tokenize string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tokenized text:\\n{}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# create text as a single string\n",
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "print('Original text:\\n{}\\n'.format(s))\n",
    "\n",
    "# tokenize string\n",
    "tokens = nltk.word_tokenize(s)\n",
    "print('Tokenized text:\\n{}\\n'.format(tokens))\n",
    "\n",
    "# POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print('POS-tagged tokens:\\n{}\\n'.format(tagged_tokens))\n",
    "\n",
    "# convert tags to WordNet form\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    \"\"\"Convert POS-tagged tokens to WordNet form tags.\"\"\"\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    # if a word does not belong to one of the 4 categories make it a NOUN\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN)) for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens\n",
    "\n",
    "# call function\n",
    "wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "print(\"WordNet-tagged tokens:\\n{}\\n\".format(wordnet_tokens))\n",
    "\n",
    "# lemmatize tokens\n",
    "lemmatized_text = \" \".join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "print(\"Lemmatized text:\\n{}\".format(lemmatized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zisZIs1JFan"
   },
   "source": [
    "### Define a function that performs lemmatization using NLTK and WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6LditBNJFao"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The brown foxes are quick and they are jumping over the sleeping lazy dogs!\n",
      "\n",
      "Lemmatized text:\n",
      "The brown fox be quick and they be jump over the sleep lazy dog !\n"
     ]
    }
   ],
   "source": [
    "def wordnet_lemmatize_text(text):\n",
    "    \"\"\"Lemmatize a single string of text.\"\"\"\n",
    "    \n",
    "    # tokenize and POS-tag tokens\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    # convert tags into WordNet tags\n",
    "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "    # lemmatize tagged tokens and join words back to a single string\n",
    "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "    # return the lemmatized string\n",
    "    return lemmatized_text\n",
    "\n",
    "# call function\n",
    "print('Original text:\\n{}\\n'.format(s))\n",
    "print('Lemmatized text:\\n{}'.format(wordnet_lemmatize_text(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zisZIs1JFan"
   },
   "source": [
    "<a name=\"SpacyProcess\"></a>\n",
    "## 1.2.2  Lemmatization with spaCy.\n",
    "**`token.lemma_`** [Info](https://spacy.io/api/lemmatizer) <br>\n",
    "**`token.text`** [Info](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3N2ExlFqJFaw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The brown foxes are quick and they are jumping over the sleeping lazy dogs!\n",
      "\n",
      "Lemmatized text:\n",
      "the brown fox be quick and they be jump over the sleep lazy dog !\n"
     ]
    }
   ],
   "source": [
    "def spacy_lemmatize_text(text):\n",
    "    # create an NLP object\n",
    "    text = nlp(text)\n",
    "    #  & join tokens back into a string\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    # return lemmatized string\n",
    "    return text\n",
    "\n",
    "# call function\n",
    "print('Original text:\\n{}\\n'.format(s))\n",
    "print('Lemmatized text:\\n{}'.format(spacy_lemmatize_text(s)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "01 - Text Wrangling Examples.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
