{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db4f16b",
   "metadata": {},
   "source": [
    "# Game \"Safety\" Classification From Game Reviews\n",
    "by [CSpanias](https://cspanias.github.io/aboutme/), 1st Week's Project for [Solving Business Problems with NLP](https://omdena.com/course/solving-business-problems-with-nlp/) by Omdena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45838b18",
   "metadata": {},
   "source": [
    "# CONTENT\n",
    "1. [Webscraping Data](#webscraping)\n",
    "1. [Functions for Text PreProcessing, Stemming \\& Model Evaluation](#functions)\n",
    "1. [Data Wrangling](#datawrangling)\n",
    "1. [NLP PipeLine](#pipeline)\n",
    "    1. [Basic NLP Count-Based Features](#CountBasedFeatures)\n",
    "    1. [Sentiment Analysis](#sentimentanalysis)\n",
    "    1. [Term Frequency-Inverse Document Frequency](#tfidf) \n",
    "    1. [Logistic Regression](#logreg)\n",
    "    1. [Random Forest Classifier](#rfc)\n",
    "        1. [Hyperparameter Tuning](#gs)\n",
    "    1. [Sentiment Analysis B](#vader)\n",
    "1. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34689e1",
   "metadata": {},
   "source": [
    "<a name=\"webscraping\"></a>\n",
    "# 1. Webscraping Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e203f523",
   "metadata": {},
   "source": [
    "The data were scraped using __[ParseHub](https://www.parsehub.com/)__, a free and straightforward tool for web scraping.\n",
    "\n",
    "Notice that in order to ensure that the games will have reviews, my __starting URL__ had the games __sorted by Stars: High to Low__:\n",
    "\n",
    "<img src=\"sort.PNG\" align='left' alt=\"sort\" style=\"width: 80%;\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "565b7886",
   "metadata": {},
   "source": [
    "In addition, reviews were __filtered by both Parent & Kids Populatiry__:\n",
    "\n",
    "<img src=\"filters.PNG\" align='left' alt=\"filters\" style=\"width: 80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2144d",
   "metadata": {},
   "source": [
    " You can see the __step-by-step commands__ used on the following GIF image.\n",
    " \n",
    " __Note__: Uncomment and render as markdown cell to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#![parsehub_commands.gif](attachment:parsehub_commands.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db35a8",
   "metadata": {},
   "source": [
    "<a name=\"functions\"></a>\n",
    "# 2. Functions for Text Pre-Processing, Stemming & Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9491b",
   "metadata": {},
   "source": [
    "First, we have to __import the required libraries__ that we aim to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bfdfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd # import dataset, create and manipulate dataframes\n",
    "\n",
    "\n",
    "\n",
    "import string # count-based features\n",
    "\n",
    "\n",
    "from pprint import pprint # pretty print\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # model\n",
    "from sklearn.ensemble import RandomForestClassifier # model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # count-based language models\n",
    "from sklearn.metrics import classification_report, make_scorer # model evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score # model evaluation metrics\n",
    "from sklearn.model_selection import GridSearchCV # split & evaluate dataset, hyperparameter optimization\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict # cross-validation\n",
    "from collections import Counter # count-based calculations\n",
    "from textblob import TextBlob # sentiment analysis\n",
    "\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # hide warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fc99f",
   "metadata": {},
   "source": [
    "First, in order to __avoid repetitive code chunks__, we will create some relatively simple functions to user later during our NLP pipeline:\n",
    "1. `normalize_document` for basic text preprocessing tasks\n",
    "2. `simple_text_preprocessor` for stemming and some basic text preprocessing tasks\n",
    "3. `extended_classification_report` for evaluating our models\n",
    "4. `generate_confusion_matrix` for visualizing our results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a71e6",
   "metadata": {},
   "source": [
    "The `normalize_document` function aims to perform some basic text pre-processing tasks in any text document. More specifically:\n",
    "1. Remove special characters (any characters that are not alphabetic or numeric) using regular expressions.\n",
    "2. Remove trailing (at the beginning and/or the end) whitespace.\n",
    "3. Expand contracted words, e.g. `It's` &rarr; `It is`.\n",
    "4. Tokenize text (split sentences into individual words).\n",
    "5. Remove stopwords such as the, a, an, etc.\n",
    "6. Join tokens back into a single string, i.e. like it was first inputted, but \"cleaned\".\n",
    "\n",
    "__Note__: A great [article](https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a) about the differences of removing stopwords using different libraries (__NLTK__, __spaCy__, __gensim__, __scikit-learn__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4887488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions # expand contractions\n",
    "import re # regular expressions\n",
    "import numpy as np # vectorize functions and perform calculations\n",
    "from nltk.tokenize import word_tokenize # tokenize strings\n",
    "from gensim.parsing.preprocessing import STOPWORDS # removing stopwords\n",
    "\n",
    "def normalize_document(doc):\n",
    "    \"\"\"Normalize the document by performing basic text pre-processing tasks.\"\"\"\n",
    "    # remove special characters\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    # remove trailing whitespace\n",
    "    nowhite = doc.strip()\n",
    "    # expand contractions\n",
    "    expanded = contractions.fix(nowhite)\n",
    "    # tokenize document\n",
    "    tokens = word_tokenize(expanded)\n",
    "    # remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    # re-create document from tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "\n",
    "    return doc\n",
    "\n",
    "# vectorize function for faster computations\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefc05e",
   "metadata": {},
   "source": [
    "We will also make a function that performs __stemming__ to a document, that is, removing the word's __affixes__. We will use this function __prior the application of the tfidf process__ (more on this later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer # stemming\n",
    "\n",
    "def simple_text_preprocessor(document):\n",
    "    \"\"\"Perform basic text pre-processing tasks.\"\"\"\n",
    "    # load up a simple porter stemmer - nothing fancy\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # lower case\n",
    "    document = str(document).lower()\n",
    "\n",
    "    # expand contractions\n",
    "    document = contractions.fix(document)\n",
    "\n",
    "    # remove unnecessary characters\n",
    "    document = re.sub(r'[^a-zA-Z]',r' ', document)\n",
    "    document = re.sub(r'nbsp', r'', document)\n",
    "    document = re.sub(' +', ' ', document)\n",
    "\n",
    "    # simple porter stemming\n",
    "    document = ' '.join([ps.stem(word) for word in document.split()])\n",
    "\n",
    "    # stopwords removal\n",
    "    document = ' '.join([word for word in document.split() if word not in STOPWORDS])\n",
    "\n",
    "    return document\n",
    "\n",
    "# vectorize function\n",
    "stp = np.vectorize(simple_text_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d2a69",
   "metadata": {},
   "source": [
    "Next we define the `extended_classification_report` which is a function (which I am sure takes a lot of refactoring) that evaluates a model using __cross-validation__, and it is what its name says: an extension of the original __classification report__ from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5f8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def extended_classification_report(model, kf, X, y):\n",
    "       \n",
    "    # define scoring metrics\n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1', 'neg_brier_score', 'neg_log_loss', 'roc_auc']\n",
    "    \n",
    "    # cross-validate model\n",
    "    model_scores = cross_validate(model, X, y, cv=kf, scoring=scoring, return_train_score=True)\n",
    "\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    precision_splits = []\n",
    "    recall_splits = []\n",
    "    f1_splits = []\n",
    "    brier_splits = []\n",
    "    logloss_splits = []\n",
    "    rocauc_splits = []\n",
    "    for key, value in model_scores.items():\n",
    "        if key == 'train_accuracy':\n",
    "            accuracy_train.append(value)\n",
    "        if key == 'test_accuracy':\n",
    "            accuracy_test.append(value)\n",
    "        if key == 'test_precision':\n",
    "            precision_splits.append(value)\n",
    "        if key == 'test_recall':\n",
    "            recall_splits.append(value)\n",
    "        if key == 'test_f1':\n",
    "            f1_splits.append(value)\n",
    "        if key == 'test_neg_brier_score':\n",
    "            brier_splits.append(value)\n",
    "        if key == 'test_neg_log_loss':\n",
    "            logloss_splits.append(value)\n",
    "        if key == 'test_roc_auc':\n",
    "            rocauc_splits.append(value)\n",
    "\n",
    "\n",
    "    # set column names\n",
    "    split_cols_names = ['split 1', 'split 2', 'split 3', 'split 4', 'split 5',\n",
    "                        'split 6', 'split 7', 'split 8', 'split 9', 'split 10']\n",
    "\n",
    "    # convert lists of scores to dataframe\n",
    "    accuracy_train = pd.DataFrame(accuracy_train, columns=split_cols_names )\n",
    "    accuracy_test = pd.DataFrame(accuracy_test, columns=split_cols_names)\n",
    "    precision_splits = pd.DataFrame(precision_splits, columns=split_cols_names)\n",
    "    recall_splits = pd.DataFrame(recall_splits, columns=split_cols_names)\n",
    "    f1_splits = pd.DataFrame(f1_splits, columns=split_cols_names)\n",
    "    brier_splits = pd.DataFrame(brier_splits, columns=split_cols_names)\n",
    "    logloss_splits = pd.DataFrame(logloss_splits, columns=split_cols_names)\n",
    "    rocauc_splits = pd.DataFrame(rocauc_splits, columns=split_cols_names)\n",
    "\n",
    "    # rename rows\n",
    "    accuracy_train.rename(index = {0: \"Accuracy Train\"}, inplace=True)\n",
    "    accuracy_test.rename(index = {0: \"Accuracy Test\"}, inplace=True)\n",
    "    precision_splits.rename(index = {0: \"Precision\"}, inplace = True)\n",
    "    recall_splits.rename(index = {0: \"Recall\"}, inplace = True)\n",
    "    f1_splits.rename(index = {0: \"F1\"}, inplace = True)\n",
    "    brier_splits.rename(index = {0: \"Brier\"}, inplace = True)\n",
    "    logloss_splits.rename(index = {0: \"LogLoss\"}, inplace = True)\n",
    "    rocauc_splits.rename(index = {0: \"RocAuc\"}, inplace = True)\n",
    "\n",
    "\n",
    "    # merge all dataframes into a single one\n",
    "    metrics_model = pd.concat([accuracy_train, accuracy_test, precision_splits, recall_splits, f1_splits,\n",
    "                         brier_splits, logloss_splits, rocauc_splits])\n",
    "\n",
    "    # calculate mean scores for each row\n",
    "    mean_scores = metrics_model.mean(axis=1)\n",
    "\n",
    "    # append column to the dataframe\n",
    "    metrics_model['mean'] = round(mean_scores, 4)\n",
    "    \n",
    "    # display dataframe as a table\n",
    "    return display(metrics_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a9a0a",
   "metadata": {},
   "source": [
    "Lastly, we define a short-function `generate_confusion_matrix` for visualizing our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8992a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def generate_confusion_matrix(y, y_pred):\n",
    "    \"\"\"Generate a confusion matrix based on a seaborn heatmap.\"\"\"\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    # visualize confusion matrix with seaborn heatmap\n",
    "    cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1',\n",
    "                                               'Actual Negative:0'],\n",
    "                            index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "    fig, ax = plt.subplots(figsize=(7,7))  \n",
    "    return sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628dbdf",
   "metadata": {},
   "source": [
    "<a name=\"datawrangling\"></a>\n",
    "# 2. Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ac92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/CSpanias/nlp_resources/main/nlp_omdena/w1/game_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dabb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inspect first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63237a94",
   "metadata": {},
   "source": [
    "We can see that the __column names are unecessary long__, thus, it seems like a good idea to rename them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4b63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check column names\n",
    "print(df.columns)\n",
    "\n",
    "# rename columns\n",
    "df.rename(columns={\n",
    "    'game_title_name': 'title',\n",
    "    'game_title_game_age': 'age',\n",
    "    'game_title_kid_review_name': 'review_kid',\n",
    "    'game_title_parent_review_name': 'review_parent'\n",
    "}, inplace=True)\n",
    "\n",
    "# check first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa1bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check basic stats\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4119f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(\"Number of duplicated rows: {}.\".format(df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb908f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop duplicated rows\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# check duplicates\n",
    "print(\"Number of duplicated rows: {}.\".format(df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498c5b0",
   "metadata": {},
   "source": [
    "The age column, which will form the base for our classification includes symbols `+`, whitespace, as well as the string `age`.\n",
    "\n",
    "We will __clean that up using a regular expression__ and extract only what is relevant to us, i.e. the numeric characters.\n",
    "\n",
    "We will also __replace `NaN` values with an emtpy string__ and then __convert the column to numeric__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2243242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean age column\n",
    "df.age = df.age.str.replace(pat=r'[^\\d{,2}]', repl='', regex=True)\n",
    "\n",
    "#check 1st 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values\n",
    "df.replace(np.nan,'',regex=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f267a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert age column to int\n",
    "df.age = pd.to_numeric(df.age)\n",
    "\n",
    "# check dtype of age\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d84e4d",
   "metadata": {},
   "source": [
    "We are not interested in seperating the kids' from parents' reviews in this project, thus, we will __concatenate the two in a single column__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bccd253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge reviews into 1 column\n",
    "df['reviews'] = df.review_kid + df.review_parent\n",
    "\n",
    "# discard unecessary columns\n",
    "df.drop(columns=['review_kid', 'review_parent'], inplace=True)\n",
    "\n",
    "# check 1st five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7eeae5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make every review lower-case\n",
    "df['reviews'] = df['reviews'].apply(str.lower)\n",
    "\n",
    "# check first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27145ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aca5a2",
   "metadata": {},
   "source": [
    "As we can see there are __303 missing values__ in the age column.\n",
    "\n",
    "We will fill those with __the forward fill method__, as our missing values refer to the movie before them, hence, they have the same age as the cell before them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba47935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forwardfill missing values\n",
    "df.fillna(method='ffill', axis=None, inplace=True)\n",
    "\n",
    "# check for NaNs\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84534751",
   "metadata": {},
   "source": [
    "Now, we will create our target column based on age:\n",
    "* If the game has an `age 17+` sign, we want to classify this as `non-safe` and label it as `0`, otherwise we will classify it as `safe` and label it as `1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b3033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create category of interest, 1 = safe, 0 = non-safe\n",
    "df['safe'] = df.apply(lambda row: 1 if row['age'] < 17  else 0, axis=1)\n",
    "\n",
    "# check 1st five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799298a",
   "metadata": {},
   "source": [
    "SInce we have our target column, we don't really need `age` anymore (and we never needed `title` to begin with!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard unecessary columns\n",
    "df.drop(columns=['title', 'age'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194a2b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check values\n",
    "print(\"The target distribution is {} safe (1) and {} non-safe (0) movie titles.\"\n",
    "      .format(df.safe.value_counts()[0], df.safe.value_counts()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud # visualization\n",
    "\n",
    "# generate a wordcloud for safe titles\n",
    "safe_wordcloud = WordCloud(width=512, height=512).generate(' '.join(df['reviews'][df['safe']==1]))\n",
    "plt.figure(figsize=(6, 4), facecolor='k')\n",
    "plt.imshow(safe_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b89f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate a wordcloud for non-safe titles\n",
    "non_safe_wordcloud = WordCloud(width=512, height=512).generate(' '.join(df['reviews'][df['safe']==0]))\n",
    "plt.figure(figsize=(6, 4), facecolor='k')\n",
    "plt.imshow(non_safe_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize 'reviews' column\n",
    "norm_corpus = normalize_corpus(list(df['reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9aca20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "print(\"The 'review' column has {} rows.\\n\".format(df.reviews.shape[0]))\n",
    "\n",
    "# check first 5 rows\n",
    "print(\"The first 5 reviews are:\\n{}\\n\\n\".format(df.reviews.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae295f66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# assign feature & target variables\n",
    "X = df.drop(['safe'], axis = 1)\n",
    "y = df['safe']\n",
    "\n",
    "# check shape of features & target sets\n",
    "print(\"Features' set shape: {} | Target's set shape {}.\"\n",
    "      .format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab09066",
   "metadata": {},
   "source": [
    "# 3. NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed4972",
   "metadata": {},
   "source": [
    "<a name=\"CountBasedFeatures\"></a>\n",
    "## 3.1 Basic NLP Count-Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ebba2",
   "metadata": {},
   "source": [
    "A number of basic text based features can also be created which sometimes are helpful for **improving text classification models**. \n",
    "\n",
    "Some examples are:\n",
    "\n",
    "- __Word Count:__ total number of words in the documents\n",
    "- __Character Count:__ total number of characters in the documents\n",
    "- __Average Word Density:__ average length of the words used in the documents\n",
    "- __Puncutation Count:__ total number of punctuation marks in the documents\n",
    "- __Upper Case Count:__ total number of upper count words in the documents\n",
    "- __Title Word Count:__ total number of proper case (title) words in the documents\n",
    "\n",
    "Since we chose to __lower-case our reviews__ during the text preprocessing step, we won't need the __upper-case__ & __title-case__ word count features.\n",
    "\n",
    "**Note**: The aforementioned information comes from [this](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total number of characters\n",
    "X['char_count'] = X['reviews'].apply(len)\n",
    "# calculate total number of words\n",
    "X['word_count'] = X['reviews'].apply(lambda x: len(x.split()))\n",
    "# # calculate average word density\n",
    "X['word_density'] = X['char_count'] / (X['word_count']+1)\n",
    "# calculate total number of punctuaction marks\n",
    "X['punctuation_count'] = X['reviews'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd15a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check df\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c081b0",
   "metadata": {},
   "source": [
    "<a name=\"sentimentanalysis\"></a>\n",
    "## 3.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95877d30",
   "metadata": {},
   "source": [
    "> _\"Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral.\"_ ([Lexalytics](https://www.lexalytics.com/technology/sentiment-analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39b8b0",
   "metadata": {},
   "source": [
    "We want to **infer safety from game reviews** which are higly **subjective**, **opinionated** and people often **express strong emotions** and **feelings** through it. \n",
    "\n",
    "This makes it a classic case where the text documents here are a good candidate for **extracting sentiment as a feature**.\n",
    "\n",
    "The general expectation is that a **\"safe\" review** (label 1) should have a **positive sentiment** and a **\"non-safe\" review** (label 0) should have a **negative sentiment**.\n",
    "\n",
    "**`TextBlob`** is an excellent open-source library for performing **sentiment analysis** based on a **sentiment lexicon** which leverages to give both **polarity and subjectivity scores**. \n",
    "\n",
    "* Polarity is a float that lies between \\[-1,1\\], -1 indicates negative sentiment and +1 indicates positive sentiments.\n",
    "* Subjectivity is also a float that lies in the range of \\[0,1\\]. Subjective sentences generally refer to opinion, emotion, or judgment. \n",
    "\n",
    "This is **unsupervised**, **lexicon-based sentiment analysis** where **we don't have any pre-labeled data** saying which review migth have a positive or negative sentiment. \n",
    "\n",
    "**Note**:The above information come from [this](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72) and [this](https://www.analyticsvidhya.com/blog/2021/01/sentiment-analysis-vader-or-textblob/) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93950393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate review's sentiment \n",
    "x_snt_obj = X['reviews'].apply(lambda row: TextBlob(row).sentiment)\n",
    "# create a column for polarity scores\n",
    "X['Polarity'] = [obj.polarity for obj in x_snt_obj.values]\n",
    "# create a column for subjectivity scores\n",
    "X['Subjectivity'] = [obj.subjectivity for obj in x_snt_obj.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4e07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check df\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb12e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a new column with cleaned text\n",
    "X['clean_reviews'] = stp(X['reviews'].values)\n",
    "\n",
    "# check first 5 rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abe3e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove the 2 columns\n",
    "X_metadata = X.drop(['reviews', 'clean_reviews'], axis=1).reset_index(drop=True)\n",
    "\n",
    "# check first 5 rows\n",
    "X_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdff21",
   "metadata": {},
   "source": [
    "<a name=\"tfidf\"></a>\n",
    "## 3.3 Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b54d79",
   "metadata": {},
   "source": [
    "__Term Frequency-Inverse Document Frequency__ (tf-idf) uses a combination of two metrics in\n",
    "its computation, namely: __term frequency__ (tf) and __inverse document frequency__ (idf). \n",
    "\n",
    "This technique was developed for ranking results for queries in search engines and now it is an indispensable model in the world of __information retrieval__ and NLP.\n",
    "\n",
    "__Note__: More information about [__tfidf__](https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83ddc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instatiate vectorizer\n",
    "tv = TfidfVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 1))\n",
    "\n",
    "# fit vectorizer to 'Clean Review' and convert it to numpy array\n",
    "X_tv = tv.fit_transform(X['clean_reviews']).toarray()\n",
    "# create a pandas DataFrame\n",
    "X_tv = pd.DataFrame(X_tv, columns=tv.get_feature_names())\n",
    "\n",
    "# check first 5 rows\n",
    "X_tv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233dcee",
   "metadata": {},
   "source": [
    "Now we will __concatenate the two dataframes__, the one that hold __reviews metadata__ and the one with the __tfidf scores__, into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4481df2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# concatenate the 2 dataframes\n",
    "X_comb = pd.concat([X_metadata, X_tv], axis=1)\n",
    "\n",
    "# check first 5 rows\n",
    "X_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394d237",
   "metadata": {},
   "source": [
    "<a name=\"logreg\"></a>\n",
    "## 3.4 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c71e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate log reg\n",
    "lr = LogisticRegression(C=1, random_state=42, solver='liblinear')\n",
    "\n",
    "# choose how many train/test sets we want by \"n_splits\"\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# evaluate model\n",
    "extended_classification_report(model=lr, kf=kfold, X=X_comb, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaba74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict using cv\n",
    "y_pred = cross_val_predict(lr, X_comb, y, cv=kfold)\n",
    "\n",
    "# generate cm\n",
    "generate_confusion_matrix(y=y, y_pred=y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b46bd9",
   "metadata": {},
   "source": [
    "<a name=\"rfc\"></a>\n",
    "## 3.5 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec11bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# evaluate model\n",
    "extended_classification_report(model=rfc, kf=kfold, X=X_comb, y=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0407f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict using cv\n",
    "y_pred = cross_val_predict(rfc, X_comb, y, cv=kfold)\n",
    "\n",
    "# generate cm\n",
    "generate_confusion_matrix(y=y, y_pred=y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bcdca",
   "metadata": {},
   "source": [
    "<a name=\"gs\"></a>\n",
    "## 3.5.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d939e",
   "metadata": {},
   "source": [
    "__Note__: Details about how to Tune an RF model in this [article](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d1c33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rfc.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0355471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 10, 20, 40, 60, 80, 100],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10, 15, 100],\n",
    "    'n_estimators': [100, 250, 500, 750, 1000, 1200]\n",
    "}\n",
    "\n",
    "# choose how many train/test sets we want by \"n_splits\"\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n",
    "                          cv = kfold, n_jobs = -1, verbose = 3)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_comb, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a498b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check best parameters\n",
    "print(\"Parameters suggested by GS:\\n\\n{}\".format(grid_search.best_params_))\n",
    "\n",
    "# instantiate model with best params\n",
    "best_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832719a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose how many train/test sets we want by \"n_splits\"\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# evaluate model\n",
    "extended_classification_report(model=best_grid, kf=kfold, X=X_comb, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using cv\n",
    "y_pred = cross_val_predict(best_grid, X_comb, y, cv=kfold)\n",
    "\n",
    "# generate cm\n",
    "generate_confusion_matrix(y=y, y_pred=y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ebdfcc",
   "metadata": {},
   "source": [
    "<a name=\"vader\"></a>\n",
    "# 4. Sentiment Analysis B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d4831",
   "metadata": {},
   "source": [
    "[TextBlob vs Vader](https://www.analyticsvidhya.com/blog/2021/01/sentiment-analysis-vader-or-textblob/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9b911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c3f3d33",
   "metadata": {},
   "source": [
    "<a name=\"conclusions\"></a>\n",
    "# 4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f80c2a",
   "metadata": {},
   "source": [
    "1. As we can see __Logistic Regression__ does a pretty good job with a __mean f1_score of 89%__. \n",
    "\n",
    "\n",
    "2. __Random Forest__ without any tuning performs almost perfect with a __mean f1_Score of 97%__.\n",
    "\n",
    "\n",
    "3. __Hyperparameter tuning__ using GS managed to increase the performance even more and raised the __mean f1_Score to over 99%__!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
